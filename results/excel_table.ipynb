{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aurel\\Documents\\GitHub\\Code_Thesis_GitHub\\Code_Thesis_GitHub\n"
     ]
    }
   ],
   "source": [
    "project_root = os.path.abspath(os.path.join('..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "print(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "83.2 4.88 99.04 88.72\n",
      "94.6 12.41 99.49 84.62\n",
      "92.2 13.85 99.60 83.81\n",
      "100.0 0.00 100.00 97.50\n",
      "100.0 0.00 100.00 100.00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Multiline string containing the text\n",
    "data = \"\"\"\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "\"\"\"\n",
    "\n",
    "# Extract data lines containing percentages\n",
    "lines = data.split(\"\\n\")\n",
    "data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "# Extract and print the desired values\n",
    "for line in data_lines:\n",
    "    parts = re.findall(r\"(\\d+\\.\\d+)%\\)\", line)\n",
    "    values = \" \".join(parts)\n",
    "    print(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.9\n",
      "77.3 7.30 98.61 86.06\n",
      "94.2 13.96 98.56 81.98\n",
      "94.4 13.33 98.31 82.58\n",
      "97.0 0.00 93.96 93.89\n",
      "100.0 0.00 85.71 100.00\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Extract data lines containing percentages\n",
    "lines = data.split(\"\\n\")\n",
    "data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "# Extract and print the desired values\n",
    "for line in data_lines:\n",
    "    parts = re.findall(r\"(\\d+\\.\\d+)%\\)\", line)\n",
    "    values = \" \".join(parts)\n",
    "    print(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "83.2 4.88 99.04 88.72\n",
      "94.6 12.41 99.49 84.62\n",
      "92.2 13.85 99.60 83.81\n",
      "100.0 0.00 100.00 97.50\n",
      "100.0 0.00 100.00 100.00\n",
      "\n",
      "\n",
      "6.9\n",
      "77.3 7.30 98.61 86.06\n",
      "94.2 13.96 98.56 81.98\n",
      "94.4 13.33 98.31 82.58\n",
      "97.0 0.00 93.96 93.89\n",
      "100.0 0.00 85.71 100.00\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Loop through each section, extract and print the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and print the desired values for each section\n",
    "    for line in data_lines:\n",
    "        parts = re.findall(r\"(\\d+\\.\\d+)%\\)\", line)\n",
    "        values = \" \".join(parts)\n",
    "        print(values)\n",
    "    \n",
    "    print(\"\\n\")  # Separate sections with an empty line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Create a list to store data for each section\n",
    "section_data = []\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    section_values = []\n",
    "    for line in data_lines:\n",
    "        parts = re.findall(r\"(\\d+\\.\\d+)%\\)\", line)\n",
    "        section_values.append(parts)\n",
    "    \n",
    "    section_data.append(section_values)\n",
    "\n",
    "# Create a pandas DataFrame for each experiment's data\n",
    "dfs = []\n",
    "for i, data_values in enumerate(zip(*section_data)):\n",
    "    df = pd.DataFrame(data_values, columns=[\"Value1\", \"Value2\", \"Value3\", \"Value4\"])\n",
    "    dfs.append(df)\n",
    "\n",
    "# Save each DataFrame to a separate Excel file\n",
    "for i, df in enumerate(dfs):\n",
    "    df.to_excel(f\"experiment_{i + 1}.xlsx\", index=False)\n",
    "\n",
    "\n",
    "print(dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved: c:\\Users\\aurel\\Documents\\GitHub\\Code_Thesis_GitHub\\Code_Thesis_GitHub/results/excel/experiment_1.xlsx\n",
      "Excel file saved: c:\\Users\\aurel\\Documents\\GitHub\\Code_Thesis_GitHub\\Code_Thesis_GitHub/results/excel/experiment_2.xlsx\n",
      "DataFrame is empty for experiment 3, skipping...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Create a list to store data for each section\n",
    "section_data = []\n",
    "\n",
    "# Define regular expressions to match the required patterns\n",
    "pattern = re.compile(r\"([\\d\\.]+)%\\)\")\n",
    "pattern_event = re.compile(r\"Best Configuration test on (\\d+) events from the (\\d+)th sample\")\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Find the number of events and the sample number\n",
    "    match = pattern_event.search(section)\n",
    "    events = None\n",
    "    sample = None\n",
    "    if match:\n",
    "        events, sample = match.groups()\n",
    "    \n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "    \n",
    "    section_values = []\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        section_values.append(parts)\n",
    "    \n",
    "    section_data.append((sample, events, section_values))\n",
    "\n",
    "# Create a pandas DataFrame for each experiment's data\n",
    "dfs = []\n",
    "for i, (sample, events, data_values) in enumerate(section_data):\n",
    "    df = pd.DataFrame(data_values, columns=[f\"Value1_{events}\", f\"Value2_{events}\", f\"Value3_{events}\", f\"Value4_{events}\"])\n",
    "    dfs.append(df)\n",
    "\n",
    "# Specify the file path and save each DataFrame to a separate Excel file\n",
    "for i, df in enumerate(dfs):\n",
    "    if not df.empty:\n",
    "        file_path = project_root + f\"/results/excel/experiment_{i + 1}.xlsx\"  # Specify your desired file path here\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"Excel file saved: {file_path}\")\n",
    "    else:\n",
    "        print(f\"DataFrame is empty for experiment {i + 1}, skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved: c:\\Users\\aurel\\Documents\\GitHub\\Code_Thesis_GitHub\\Code_Thesis_GitHub/results/excel/experiment_1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Create a list to store data for each section\n",
    "section_data = []\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "pattern_event = re.compile(r\"Best Configuration test on (\\d+) events from the (\\d+)th sample\")\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Find the number of events and the sample number\n",
    "    match = pattern_event.search(section)\n",
    "    events = None\n",
    "    sample = None\n",
    "    if match:\n",
    "        events, sample = match.groups()\n",
    "    \n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "    \n",
    "    # Extract values manually based on positions\n",
    "    section_values = []\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            values = parts[:4]  # Extract the first four percentages\n",
    "            section_values.append(values)\n",
    "    \n",
    "    section_data.append((sample, events, section_values))\n",
    "\n",
    "# Create a pandas DataFrame for each 2-line pair and save to separate Excel files\n",
    "num_sections = len(section_data)\n",
    "num_tables = num_sections // 2  # Calculate the number of tables\n",
    "for i in range(num_tables):\n",
    "    df1 = pd.DataFrame(section_data[2*i][2], columns=[f\"Value1_{section_data[2*i][1]}\", f\"Value2_{section_data[2*i][1]}\", f\"Value3_{section_data[2*i][1]}\", f\"Value4_{section_data[2*i][1]}\"])\n",
    "    df2 = pd.DataFrame(section_data[2*i + 1][2], columns=[f\"Value1_{section_data[2*i + 1][1]}\", f\"Value2_{section_data[2*i + 1][1]}\", f\"Value3_{section_data[2*i + 1][1]}\", f\"Value4_{section_data[2*i + 1][1]}\"])\n",
    "    combined_df = pd.concat([df1, df2], axis=1)\n",
    "    if not combined_df.empty:\n",
    "        file_path = project_root + f\"/results/excel/experiment_{i + 1}.xlsx\"  # Specify your desired file path here\n",
    "        combined_df.to_excel(file_path, index=False)\n",
    "        print(f\"Excel file saved: {file_path}\")\n",
    "    else:\n",
    "        print(f\"DataFrame is empty for table {i + 1}, skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Value5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m         metric_key \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         experiment_data \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m{\u001b[39;00msample\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mvalues]\n\u001b[1;32m---> 91\u001b[0m         metrics_data[metric_key]\u001b[39m.\u001b[39mappend(experiment_data)\n\u001b[0;32m     93\u001b[0m \u001b[39m# Create a pandas DataFrame for each metric and save to separate Excel files\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m metric, data \u001b[39min\u001b[39;00m metrics_data\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Value5'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Create a list to store data for each section\n",
    "section_data = []\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "pattern_event = re.compile(r\"Best Configuration test on (\\d+) events from the (\\d+)th sample\")\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Find the number of events and the sample number\n",
    "    match = pattern_event.search(section)\n",
    "    events = None\n",
    "    sample = None\n",
    "    if match:\n",
    "        events, sample = match.groups()\n",
    "    \n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "    \n",
    "    # Extract values manually based on positions\n",
    "    section_values = []\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            values = parts[:4]  # Extract the first four percentages\n",
    "            section_values.append(values)\n",
    "    \n",
    "    section_data.append((sample, events, section_values))\n",
    "\n",
    "# Create a dictionary to store data for each metric\n",
    "metrics_data = {f\"Value{i}\": [] for i in range(1, 5)}\n",
    "\n",
    "# Fill in the data for each metric\n",
    "for sample, events, data_values in section_data:\n",
    "    for i, values in enumerate(data_values):\n",
    "        metric_key = f\"Value{i+1}\"\n",
    "        experiment_data = [f\"Experiment {sample}\", *values]\n",
    "        metrics_data[metric_key].append(experiment_data)\n",
    "\n",
    "# Create a pandas DataFrame for each metric and save to separate Excel files\n",
    "for metric, data in metrics_data.items():\n",
    "    if data:\n",
    "        df = pd.DataFrame(data, columns=[\"Experiment\", \"Value1\", \"Value2\", \"Value3\", \"Value4\"])\n",
    "        file_path = project_root + f\"/results/excel/experiment_{i + 1}.xlsx\"  # Specify your desired file path here\n",
    "        df.to_excel(file_path, index=False)\n",
    "        print(f\"Excel file saved: {file_path}\")\n",
    "    else:\n",
    "        print(f\"No data for metric: {metric}, skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Value5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m         metric_key \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValue\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         experiment_data \u001b[39m=\u001b[39m [\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m{\u001b[39;00msample\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39mvalues]\n\u001b[1;32m---> 90\u001b[0m         metrics_data[metric_key]\u001b[39m.\u001b[39mappend(experiment_data)\n\u001b[0;32m     92\u001b[0m \u001b[39m# Fill in 'N/A' for missing values\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m metric, data \u001b[39min\u001b[39;00m metrics_data\u001b[39m.\u001b[39mitems():\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Value5'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Create a list to store data for each section\n",
    "section_data = []\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "pattern_event = re.compile(r\"Best Configuration test on (\\d+) events from the (\\d+)th sample\")\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Find the number of events and the sample number\n",
    "    match = pattern_event.search(section)\n",
    "    events = None\n",
    "    sample = None\n",
    "    if match:\n",
    "        events, sample = match.groups()\n",
    "    \n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "    \n",
    "    # Extract values manually based on positions\n",
    "    section_values = []\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            values = parts[:4]  # Extract the first four percentages\n",
    "            section_values.append(values)\n",
    "    \n",
    "    section_data.append((sample, events, section_values))\n",
    "\n",
    "# Create a dictionary to store data for each metric\n",
    "metrics_data = {f\"Value{i}\": [] for i in range(1, 5)}\n",
    "\n",
    "# Fill in the data for each metric\n",
    "for sample, events, data_values in section_data:\n",
    "    for i, values in enumerate(data_values):\n",
    "        metric_key = f\"Value{i+1}\"\n",
    "        experiment_data = [f\"Experiment {sample}\", *values]\n",
    "        metrics_data[metric_key].append(experiment_data)\n",
    "\n",
    "# Fill in 'N/A' for missing values\n",
    "for metric, data in metrics_data.items():\n",
    "    for i in range(len(data)):\n",
    "        if len(data[i]) < 5:\n",
    "            data[i].extend(['N/A'] * (5 - len(data[i])))\n",
    "\n",
    "# Create a pandas DataFrame for each metric and save to separate Excel files\n",
    "for metric, data in metrics_data.items():\n",
    "    df = pd.DataFrame(data, columns=[\"Experiment\", \"Value1\", \"Value2\", \"Value3\", \"Value4\"])\n",
    "    file_path = project_root + f\"/results/excel/experiment_{i + 1}.xlsx\"  # Specify your desired file path here\n",
    "    df.to_excel(file_path, index=False)\n",
    "    print(f\"Excel file saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m extracted_values:\n\u001b[0;32m     79\u001b[0m         particle_type \u001b[39m=\u001b[39m lines[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip()  \u001b[39m# Get the particle type\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m         particle_data[particle_type]\u001b[39m.\u001b[39mappend(extracted_values)\n\u001b[0;32m     82\u001b[0m \u001b[39m# Create separate DataFrames for each particle category\u001b[39;00m\n\u001b[0;32m     83\u001b[0m dfs \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "# ... (your data)\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create a dictionary to store particle data\n",
    "particle_data = {\n",
    "    \"velo\": [],\n",
    "    \"long\": [],\n",
    "    \"long>5GeV\": [],\n",
    "    \"long_strange\": [],\n",
    "    \"long_strange>5GeV\": []\n",
    "}\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Initialize a list to store extracted values\n",
    "    extracted_values = []\n",
    "\n",
    "    # Extract values manually based on positions\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            extracted_values.extend(parts[:4])\n",
    "\n",
    "    # Append extracted values to the appropriate particle category\n",
    "    if extracted_values:\n",
    "        particle_type = lines[1].strip()  # Get the particle type\n",
    "        particle_data[particle_type].append(extracted_values)\n",
    "\n",
    "# Create separate DataFrames for each particle category\n",
    "dfs = []\n",
    "for particle_type, values_list in particle_data.items():\n",
    "    for i in range(0, len(values_list), 2):\n",
    "        df = pd.DataFrame(values_list[i:i + 2], columns=[\"Value1\", \"Value2\", \"Value3\", \"Value4\"])\n",
    "        df[\"Experiment\"] = [f\"Experiment {i+1}\", f\"Experiment {i+2}\"]\n",
    "        df[\"Particle Type\"] = particle_type\n",
    "        dfs.append(df)\n",
    "\n",
    "# Combine all DataFrames into a single DataFrame\n",
    "final_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the final DataFrame to an Excel file\n",
    "final_df.to_excel(file_path, index=False)\n",
    "print(f\"Excel file saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m particle_type:\n\u001b[0;32m     71\u001b[0m         values \u001b[39m=\u001b[39m [re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m%\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m, line) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m data_lines]\n\u001b[1;32m---> 72\u001b[0m         metrics_data[particle_type]\u001b[39m.\u001b[39mextend(values)\n\u001b[0;32m     74\u001b[0m \u001b[39m# Create a pandas DataFrame for each metric's data\u001b[39;00m\n\u001b[0;32m     75\u001b[0m dfs \u001b[39m=\u001b[39m {}\n",
      "\u001b[1;31mKeyError\u001b[0m: 'velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ... (your data)\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Initialize a dictionary to store the data for each metric\n",
    "metrics_data = {\"velo\": [], \"long\": [], \"long>5GeV\": [], \"long_strange\": [], \"long_strange>5GeV\": []}\n",
    "\n",
    "# Loop through each section, extract the desired values and store in the metrics_data dictionary\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract the particle type\n",
    "    particle_type = None\n",
    "    for line in lines:\n",
    "        if re.search(r'\\b(?:velo|long|long>5GeV|long_strange|long_strange>5GeV)\\b', line):\n",
    "            particle_type = line.strip()\n",
    "            break\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if particle_type:\n",
    "        values = [re.findall(r\"(\\d+\\.\\d+)%\\)\", line) for line in data_lines]\n",
    "        metrics_data[particle_type].extend(values)\n",
    "\n",
    "# Create a pandas DataFrame for each metric's data\n",
    "dfs = {}\n",
    "for metric, values in metrics_data.items():\n",
    "    columns = [\"Value1\", \"Value2\", \"Value3\", \"Value4\"]\n",
    "    df = pd.DataFrame(values, columns=columns)\n",
    "    dfs[metric] = df\n",
    "\n",
    "# Save each metric DataFrame to an Excel file\n",
    "for metric, df in dfs.items():\n",
    "    file_path = file_path = project_root + f\"/results/excel/{metric}_table.xlsx\"  # Specify your desired file path here\n",
    "    df.to_excel(file_path, index=False)\n",
    "    print(f\"Excel file saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "6 columns passed, passed data had 23 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:568\u001b[0m, in \u001b[0;36m_list_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 568\u001b[0m     columns \u001b[39m=\u001b[39m _validate_or_indexify_columns(content, columns)\n\u001b[0;32m    569\u001b[0m     result \u001b[39m=\u001b[39m _convert_object_array(content, dtype\u001b[39m=\u001b[39mdtype, coerce_float\u001b[39m=\u001b[39mcoerce_float)\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:692\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi_list \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(columns) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(content):  \u001b[39m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     \u001b[39m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    693\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(columns)\u001b[39m}\u001b[39;00m\u001b[39m columns passed, passed data had \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    694\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(content)\u001b[39m}\u001b[39;00m\u001b[39m columns\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    695\u001b[0m     )\n\u001b[0;32m    696\u001b[0m \u001b[39melif\u001b[39;00m is_mi_list:\n\u001b[0;32m    697\u001b[0m \n\u001b[0;32m    698\u001b[0m     \u001b[39m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 6 columns passed, passed data had 23 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     88\u001b[0m columns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mExperiment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mParticle Type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mValue1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mValue2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mValue3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mValue4\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m---> 89\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(all_values, columns\u001b[39m=\u001b[39;49mcolumns)\n\u001b[0;32m     91\u001b[0m \u001b[39m# Split the DataFrame into separate DataFrames for each particle type\u001b[39;00m\n\u001b[0;32m     92\u001b[0m particle_dfs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:570\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m is_named_tuple(data[\u001b[39m0\u001b[39m]) \u001b[39mand\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m     columns \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_fields\n\u001b[1;32m--> 570\u001b[0m arrays, columns \u001b[39m=\u001b[39m to_arrays(data, columns, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    571\u001b[0m columns \u001b[39m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    573\u001b[0m \u001b[39m# set the index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:528\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[39mreturn\u001b[39;00m [], []  \u001b[39m# columns if columns is not None else []\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 528\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_to_arrays(data, columns, coerce_float\u001b[39m=\u001b[39;49mcoerce_float, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    529\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data[\u001b[39m0\u001b[39m], abc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m _list_of_dict_to_arrays(\n\u001b[0;32m    531\u001b[0m         data, columns, coerce_float\u001b[39m=\u001b[39mcoerce_float, dtype\u001b[39m=\u001b[39mdtype\n\u001b[0;32m    532\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\construction.py:571\u001b[0m, in \u001b[0;36m_list_to_arrays\u001b[1;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[0;32m    569\u001b[0m     result \u001b[39m=\u001b[39m _convert_object_array(content, dtype\u001b[39m=\u001b[39mdtype, coerce_float\u001b[39m=\u001b[39mcoerce_float)\n\u001b[0;32m    570\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 571\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[39mreturn\u001b[39;00m result, columns\n",
      "\u001b[1;31mValueError\u001b[0m: 6 columns passed, passed data had 23 columns"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ... (your data)\n",
    "\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expressions to match lines containing percentages and particle type\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "particle_type_pattern = re.compile(r\"\\b(?:velo|long|long>5GeV|long_strange|long_strange>5GeV)\\b\")\n",
    "\n",
    "# Create a list to store all extracted values\n",
    "all_values = []\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract the particle type\n",
    "    particle_type = None\n",
    "    for line in lines:\n",
    "        match = particle_type_pattern.search(line)\n",
    "        if match:\n",
    "            particle_type = match.group()\n",
    "            break\n",
    "\n",
    "    # Initialize a list to store extracted values\n",
    "    extracted_values = []\n",
    "\n",
    "    # Extract values manually based on positions\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            extracted_values.extend(parts[:4])\n",
    "\n",
    "    # Append extracted values to the list\n",
    "    if extracted_values and particle_type:\n",
    "        all_values.append([\"Experiment\", particle_type, *extracted_values])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = [\"Experiment\", \"Particle Type\", \"Value1\", \"Value2\", \"Value3\", \"Value4\"]\n",
    "df = pd.DataFrame(all_values, columns=columns)\n",
    "\n",
    "# Split the DataFrame into separate DataFrames for each particle type\n",
    "particle_dfs = {}\n",
    "for particle_type in df[\"Particle Type\"].unique():\n",
    "    particle_dfs[particle_type] = df[df[\"Particle Type\"] == particle_type]\n",
    "\n",
    "# Save each particle DataFrame to an Excel file\n",
    "for particle_type, particle_df in particle_dfs.items():\n",
    "    file_path = project_root + f\"/results/excel/{particle_type}_table.xlsx\"  # Specify your desired file path here\n",
    "    particle_df.to_excel(file_path, index=False)\n",
    "    print(f\"Excel file saved: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m parts \u001b[39m=\u001b[39m pattern\u001b[39m.\u001b[39mfindall(line)\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m parts:\n\u001b[1;32m---> 71\u001b[0m     reconstruction_efficiency, percentage_clone, purity, hit_efficiency \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mfloat\u001b[39m, parts)\n\u001b[0;32m     72\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(reconstruction_efficiency_data) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     73\u001b[0m     reconstruction_efficiency_data\u001b[39m.\u001b[39msetdefault(key, [])\u001b[39m.\u001b[39mappend(reconstruction_efficiency)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Create dictionaries to store data for each metric\n",
    "reconstruction_efficiency_data = {}\n",
    "percentage_clone_data = {}\n",
    "purity_data = {}\n",
    "hit_efficiency_data = {}\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            reconstruction_efficiency, percentage_clone, purity, hit_efficiency = map(float, parts)\n",
    "            key = f\"Experiment {len(reconstruction_efficiency_data) + 1}\"\n",
    "            reconstruction_efficiency_data.setdefault(key, []).append(reconstruction_efficiency)\n",
    "            percentage_clone_data.setdefault(key, []).append(percentage_clone)\n",
    "            purity_data.setdefault(key, []).append(purity)\n",
    "            hit_efficiency_data.setdefault(key, []).append(hit_efficiency)\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data)\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data)\n",
    "purity_df = pd.DataFrame(purity_data)\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data)\n",
    "\n",
    "# Print the DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.0']\n",
      "['83.2', '4.88', '99.04', '88.72']\n",
      "['94.6', '12.41', '99.49', '84.62']\n",
      "['92.2', '13.85', '99.60', '83.81']\n",
      "['100.0', '0.00', '100.00', '97.50']\n",
      "['100.0', '0.00', '100.00', '100.00']\n",
      "\n",
      "\n",
      "['6.9']\n",
      "['77.3', '7.30', '98.61', '86.06']\n",
      "['94.2', '13.96', '98.56', '81.98']\n",
      "['94.4', '13.33', '98.31', '82.58']\n",
      "['97.0', '0.00', '93.96', '93.89']\n",
      "['100.0', '0.00', '85.71', '100.00']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Loop through each section, extract and print the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and print the desired values for each section\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            print(parts)\n",
    "\n",
    "    print(\"\\n\")  # Separate sections with an empty line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mif\u001b[39;00m parts:\n\u001b[0;32m     72\u001b[0m     reconstruction_efficiency_data\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mReconstruction Efficiency\u001b[39m\u001b[39m'\u001b[39m, [])\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(parts[\u001b[39m0\u001b[39m]))\n\u001b[1;32m---> 73\u001b[0m     percentage_clone_data\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mPercentage Clone\u001b[39m\u001b[39m'\u001b[39m, [])\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(parts[\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m     74\u001b[0m     purity_data\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mPurity\u001b[39m\u001b[39m'\u001b[39m, [])\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(parts[\u001b[39m2\u001b[39m]))\n\u001b[0;32m     75\u001b[0m     hit_efficiency_data\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mHit Efficiency\u001b[39m\u001b[39m'\u001b[39m, [])\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(parts[\u001b[39m3\u001b[39m]))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create dictionaries to store data for each metric\n",
    "reconstruction_efficiency_data = {}\n",
    "percentage_clone_data = {}\n",
    "purity_data = {}\n",
    "hit_efficiency_data = {}\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if parts:\n",
    "            reconstruction_efficiency_data.setdefault('Reconstruction Efficiency', []).append(float(parts[0]))\n",
    "            percentage_clone_data.setdefault('Percentage Clone', []).append(float(parts[1]))\n",
    "            purity_data.setdefault('Purity', []).append(float(parts[2]))\n",
    "            hit_efficiency_data.setdefault('Hit Efficiency', []).append(float(parts[3]))\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data)\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data)\n",
    "purity_df = pd.DataFrame(purity_data)\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data)\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "   Reconstruction Efficiency\n",
      "0                       83.2\n",
      "1                       94.6\n",
      "2                       92.2\n",
      "3                      100.0\n",
      "4                      100.0\n",
      "5                       77.3\n",
      "6                       94.2\n",
      "7                       94.4\n",
      "8                       97.0\n",
      "9                      100.0\n",
      "\n",
      "Percentage Clone DataFrame:\n",
      "   Percentage Clone\n",
      "0              4.88\n",
      "1             12.41\n",
      "2             13.85\n",
      "3              0.00\n",
      "4              0.00\n",
      "5              7.30\n",
      "6             13.96\n",
      "7             13.33\n",
      "8              0.00\n",
      "9              0.00\n",
      "\n",
      "Purity DataFrame:\n",
      "   Purity\n",
      "0   99.04\n",
      "1   99.49\n",
      "2   99.60\n",
      "3  100.00\n",
      "4  100.00\n",
      "5   98.61\n",
      "6   98.56\n",
      "7   98.31\n",
      "8   93.96\n",
      "9   85.71\n",
      "\n",
      "Hit Efficiency DataFrame:\n",
      "   Hit Efficiency\n",
      "0           88.72\n",
      "1           84.62\n",
      "2           83.81\n",
      "3           97.50\n",
      "4          100.00\n",
      "5           86.06\n",
      "6           81.98\n",
      "7           82.58\n",
      "8           93.89\n",
      "9          100.00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create dictionaries to store data for each metric\n",
    "reconstruction_efficiency_data = {}\n",
    "percentage_clone_data = {}\n",
    "purity_data = {}\n",
    "hit_efficiency_data = {}\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    for line in data_lines:\n",
    "        parts = pattern.findall(line)\n",
    "        if len(parts) >= 4:\n",
    "            reconstruction_efficiency_data.setdefault('Reconstruction Efficiency', []).append(float(parts[0]))\n",
    "            percentage_clone_data.setdefault('Percentage Clone', []).append(float(parts[1]))\n",
    "            purity_data.setdefault('Purity', []).append(float(parts[2]))\n",
    "            hit_efficiency_data.setdefault('Hit Efficiency', []).append(float(parts[3]))\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data)\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data)\n",
    "purity_df = pd.DataFrame(purity_data)\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data)\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "     Experiment Value1\n",
      "0  Experiment 5    4.0\n",
      "1  Experiment 5    6.9\n",
      "\n",
      "Percentage Clone DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 5   83.2   4.88  99.04  88.72\n",
      "1  Experiment 5   77.3   7.30  98.61  86.06\n",
      "\n",
      "Purity DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 5   94.6  12.41  99.49  84.62\n",
      "1  Experiment 5   94.2  13.96  98.56  81.98\n",
      "\n",
      "Hit Efficiency DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 5   92.2  13.85  99.60  83.81\n",
      "1  Experiment 5   94.4  13.33  98.31  82.58\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create lists to store data for each metric\n",
    "reconstruction_efficiency_data = []\n",
    "percentage_clone_data = []\n",
    "purity_data = []\n",
    "hit_efficiency_data = []\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if len(data_lines) == 6:  # Ensure that there are enough lines in the section\n",
    "        reconstruction_efficiency_values = pattern.findall(data_lines[0])\n",
    "        percentage_clone_values = pattern.findall(data_lines[1])\n",
    "        purity_values = pattern.findall(data_lines[2])\n",
    "        hit_efficiency_values = pattern.findall(data_lines[3])\n",
    "        \n",
    "        if len(reconstruction_efficiency_values) == 1:\n",
    "            reconstruction_efficiency_data.append([f\"Experiment {i+1}\", *reconstruction_efficiency_values])\n",
    "        if len(percentage_clone_values) == 4:\n",
    "            percentage_clone_data.append([f\"Experiment {i+1}\", *percentage_clone_values])\n",
    "        if len(purity_values) == 4:\n",
    "            purity_data.append([f\"Experiment {i+1}\", *purity_values])\n",
    "        if len(hit_efficiency_values) == 4:\n",
    "            hit_efficiency_data.append([f\"Experiment {i+1}\", *hit_efficiency_values])\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data, columns=['Experiment', 'Value1'])\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "purity_df = pd.DataFrame(purity_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "     Experiment Value1\n",
      "0  Experiment 1    4.0\n",
      "1  Experiment 1    6.9\n",
      "\n",
      "Percentage Clone DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 1   83.2   4.88  99.04  88.72\n",
      "1  Experiment 1   77.3   7.30  98.61  86.06\n",
      "\n",
      "Purity DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 1   94.6  12.41  99.49  84.62\n",
      "1  Experiment 1   94.2  13.96  98.56  81.98\n",
      "\n",
      "Hit Efficiency DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 1   92.2  13.85  99.60  83.81\n",
      "1  Experiment 1   94.4  13.33  98.31  82.58\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data =  \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 1th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 1th sample of minibias dataset (Samples_51_to_663_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 2 mins 17.60 seconds.\n",
    "Average number of iterations per convergence: 17.8 iterations. \n",
    "\n",
    "ID of each event: [355, 39, 933, 70, 640, 201, 125, 314, 946, 957]\n",
    "Number of hits by event: [583, 484, 462, 340, 383, 503, 426, 603, 599, 541] \n",
    "Number of max_neurons by event: [361, 324, 361, 224, 182, 378, 256, 462, 756, 324] \n",
    "Hopfield networks runtime by event: ['00:00:19', '00:00:09', '00:00:16', '00:00:06', '00:00:05', '00:00:09', '00:00:08', '00:00:18', '00:00:24', '00:00:19'] \n",
    "\n",
    "619 tracks including       25 ghosts (  4.0%). Event average   4.0%\n",
    "              velo :      512 from      622 ( 82.3%,  83.2%)       25 clones (  4.88%), purity: ( 99.02%,  99.04%),  hitEff: ( 88.68%,  88.72%)\n",
    "              long :      145 from      153 ( 94.8%,  94.6%)       18 clones ( 12.41%), purity: ( 99.42%,  99.49%),  hitEff: ( 86.96%,  84.62%)\n",
    "         long>5GeV :       65 from       70 ( 92.9%,  92.2%)        9 clones ( 13.85%), purity: ( 99.53%,  99.60%),  hitEff: ( 85.49%,  83.81%)\n",
    "      long_strange :        8 from        8 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: ( 97.50%,  97.50%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: (100.00%, 100.00%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 2th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 2th sample of minibias dataset (Samples_664_to_978_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 0.48, 'T_decay': <function <lambda> at 0x7f60e98f5550>, 'B_decay': <function <lambda> at 0x7f60e98f55e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 10 mins 35.34 seconds.\n",
    "Average number of iterations per convergence: 27.75 iterations. \n",
    "\n",
    "ID of each event: [98, 354, 548, 839, 177, 515, 637, 872, 335, 941]\n",
    "Number of hits by event: [682, 901, 917, 726, 801, 967, 824, 721, 976, 970] \n",
    "Number of max_neurons by event: [528, 840, 676, 650, 552, 750, 650, 380, 961, 810] \n",
    "Hopfield networks runtime by event: ['00:00:40', '00:01:19', '00:01:08', '00:00:34', '00:00:46', '00:01:33', '00:00:54', '00:00:28', '00:01:38', '00:01:29'] \n",
    "\n",
    "1005 tracks including       69 ghosts (  6.9%). Event average   6.8%\n",
    "              velo :      808 from     1071 ( 75.4%,  77.3%)       59 clones (  7.30%), purity: ( 98.47%,  98.61%),  hitEff: ( 86.02%,  86.06%)\n",
    "              long :      265 from      289 ( 91.7%,  94.2%)       37 clones ( 13.96%), purity: ( 98.60%,  98.56%),  hitEff: ( 82.90%,  81.98%)\n",
    "         long>5GeV :      180 from      194 ( 92.8%,  94.4%)       24 clones ( 13.33%), purity: ( 98.29%,  98.31%),  hitEff: ( 83.65%,  82.58%)\n",
    "      long_strange :        9 from       11 ( 81.8%,  97.0%)        0 clones (  0.00%), purity: ( 95.97%,  93.96%),  hitEff: ( 93.61%,  93.89%)\n",
    " long_strange>5GeV :        3 from        3 (100.0%, 100.0%)        0 clones (  0.00%), purity: ( 90.48%,  85.71%),  hitEff: (100.00%, 100.00%)\n",
    "____________________\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create lists to store data for each metric\n",
    "reconstruction_efficiency_data = []\n",
    "percentage_clone_data = []\n",
    "purity_data = []\n",
    "hit_efficiency_data = []\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if len(data_lines) == 6:  # Ensure that there are enough lines in the section\n",
    "        reconstruction_efficiency_values = pattern.findall(data_lines[0])\n",
    "        percentage_clone_values = pattern.findall(data_lines[1])\n",
    "        purity_values = pattern.findall(data_lines[2])\n",
    "        hit_efficiency_values = pattern.findall(data_lines[3])\n",
    "        \n",
    "        if len(reconstruction_efficiency_values) == 1:\n",
    "            reconstruction_efficiency_data.append([f\"Experiment {i+1}\", *reconstruction_efficiency_values])\n",
    "        if len(percentage_clone_values) == 4:\n",
    "            percentage_clone_data.append([f\"Experiment {i+1}\", *percentage_clone_values])\n",
    "        if len(purity_values) == 4:\n",
    "            purity_data.append([f\"Experiment {i+1}\", *purity_values])\n",
    "        if len(hit_efficiency_values) == 4:\n",
    "            hit_efficiency_data.append([f\"Experiment {i+1}\", *hit_efficiency_values])\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data, columns=['Experiment', 'Value1'])\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "purity_df = pd.DataFrame(purity_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "     Experiment Value1\n",
      "0  Experiment 1    8.9\n",
      "1  Experiment 1    9.4\n",
      "2  Experiment 1   10.5\n",
      "\n",
      "Percentage Clone DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "\n",
      "Purity DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "\n",
      "Hit Efficiency DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 6th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 6th sample of minibias dataset (Samples_1819_to_2119_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1.24, 'T_decay': <function <lambda> at 0x7fbcf1eb55e0>, 'B_decay': <function <lambda> at 0x7fbcf1eb5670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 157 mins 25.64 seconds.\n",
    "Average number of iterations per convergence: 46.65 iterations. \n",
    "\n",
    "ID of each event: [395, 971, 786, 479, 10, 156, 353, 216, 498, 711]\n",
    "Number of hits by event: [1898, 1824, 2089, 1989, 1874, 1907, 2056, 1915, 1988, 2080] \n",
    "Number of max_neurons by event: [2700, 2254, 6000, 2860, 3480, 3968, 2805, 2856, 3190, 4032] \n",
    "Hopfield networks runtime by event: ['00:12:13', '00:11:45', '00:18:36', '00:14:20', '00:12:07', '00:21:40', '00:16:21', '00:14:42', '00:17:35', '00:18:00'] \n",
    "\n",
    "2158 tracks including      191 ghosts (  8.9%). Event average   8.8%\n",
    "              velo :     1586 from     2340 ( 67.8%,  66.6%)      171 clones ( 10.78%), purity: ( 98.19%,  98.18%),  hitEff: ( 77.24%,  77.24%)\n",
    "              long :      585 from      664 ( 88.1%,  86.3%)      109 clones ( 18.63%), purity: ( 98.43%,  98.51%),  hitEff: ( 72.67%,  72.48%)\n",
    "         long>5GeV :      377 from      421 ( 89.5%,  88.7%)       78 clones ( 20.69%), purity: ( 98.16%,  98.28%),  hitEff: ( 71.58%,  71.20%)\n",
    "      long_strange :       31 from       39 ( 79.5%,  76.0%)        5 clones ( 16.13%), purity: ( 97.22%,  98.07%),  hitEff: ( 71.07%,  71.67%)\n",
    " long_strange>5GeV :       18 from       23 ( 78.3%,  91.1%)        2 clones ( 11.11%), purity: ( 96.00%,  96.72%),  hitEff: ( 75.95%,  75.93%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 7th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 7th sample of minibias dataset (Samples_2121_to_2464_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7fbf79017550>, 'B_decay': <function <lambda> at 0x7fbf790175e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 234 mins 43.82 seconds.\n",
    "Average number of iterations per convergence: 50.9 iterations. \n",
    "\n",
    "ID of each event: [383, 95, 547, 217, 299, 721, 430, 105, 496, 1]\n",
    "Number of hits by event: [2366, 2319, 2399, 2418, 2250, 2157, 2123, 2180, 2372, 2422] \n",
    "Number of max_neurons by event: [2970, 3024, 3348, 4032, 3906, 2940, 3480, 3024, 3420, 3306] \n",
    "Hopfield networks runtime by event: ['00:24:05', '00:21:56', '00:27:21', '00:31:19', '00:23:19', '00:18:06', '00:17:00', '00:20:40', '00:23:50', '00:27:02'] \n",
    "\n",
    "2558 tracks including      240 ghosts (  9.4%). Event average   9.4%\n",
    "              velo :     1829 from     2714 ( 67.4%,  67.8%)      239 clones ( 13.07%), purity: ( 98.25%,  98.24%),  hitEff: ( 75.98%,  76.01%)\n",
    "              long :      657 from      728 ( 90.2%,  92.0%)      143 clones ( 21.77%), purity: ( 98.49%,  98.45%),  hitEff: ( 70.41%,  70.44%)\n",
    "         long>5GeV :      417 from      460 ( 90.7%,  91.5%)      101 clones ( 24.22%), purity: ( 98.53%,  98.50%),  hitEff: ( 69.31%,  69.15%)\n",
    "      long_strange :       28 from       34 ( 82.4%,  89.4%)        6 clones ( 21.43%), purity: ( 99.58%,  99.52%),  hitEff: ( 68.61%,  70.57%)\n",
    " long_strange>5GeV :       14 from       17 ( 82.4%,  91.4%)        5 clones ( 35.71%), purity: ( 99.25%,  98.21%),  hitEff: ( 70.74%,  63.83%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 8th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 8th sample of minibias dataset (Samples_2468_to_2853_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7ff8ab0c1550>, 'B_decay': <function <lambda> at 0x7ff8ab0c15e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 420 mins 12.23 seconds.\n",
    "Average number of iterations per convergence: 51.900000000000006 iterations. \n",
    "\n",
    "ID of each event: [387, 357, 642, 694, 413, 186, 277, 723, 388, 438]\n",
    "Number of hits by event: [2621, 2769, 2823, 2478, 2643, 2728, 2623, 2702, 2547, 2523] \n",
    "Number of max_neurons by event: [6474, 5328, 5037, 3660, 7387, 5183, 3780, 5550, 6474, 4356] \n",
    "Hopfield networks runtime by event: ['00:46:43', '00:42:54', '00:46:03', '00:27:54', '01:03:20', '00:46:26', '00:35:52', '00:41:26', '00:38:18', '00:31:12'] \n",
    "\n",
    "2862 tracks including      264 ghosts (  9.2%). Event average   9.2%\n",
    "              velo :     2092 from     3231 ( 64.7%,  64.8%)      279 clones ( 13.34%), purity: ( 98.26%,  98.27%),  hitEff: ( 75.25%,  75.17%)\n",
    "              long :      750 from      879 ( 85.3%,  85.3%)      181 clones ( 24.13%), purity: ( 98.49%,  98.50%),  hitEff: ( 69.07%,  68.54%)\n",
    "         long>5GeV :      528 from      584 ( 90.4%,  89.8%)      147 clones ( 27.84%), purity: ( 98.46%,  98.46%),  hitEff: ( 66.56%,  65.92%)\n",
    "      long_strange :       47 from       57 ( 82.5%,  81.0%)        4 clones (  8.51%), purity: ( 99.65%,  99.56%),  hitEff: ( 83.51%,  80.23%)\n",
    " long_strange>5GeV :       22 from       25 ( 88.0%,  82.4%)        4 clones ( 18.18%), purity: (100.00%, 100.00%),  hitEff: ( 78.81%,  74.02%)\n",
    "        long_fromb :        0 from        1 (  0.0%,   0.0%)        0 clones (  0.00%), purity: (  0.00%,   0.00%),  hitEff: (  0.00%,   0.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 9th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 9th sample of minibias dataset (Samples_2854_to_3405_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 723 mins 9.39 seconds.\n",
    "Average number of iterations per convergence: 56.8 iterations. \n",
    "\n",
    "ID of each event: [504, 869, 326, 797, 836, 727, 594, 753, 280, 607]\n",
    "Number of hits by event: [3304, 3008, 3350, 2957, 2940, 3375, 2879, 2992, 3157, 2951] \n",
    "Number of max_neurons by event: [7656, 5402, 7917, 6806, 5100, 8366, 6776, 5293, 7138, 6560] \n",
    "Hopfield networks runtime by event: ['01:36:38', '01:06:27', '01:26:27', '01:00:10', '01:00:00', '01:36:06', '00:54:56', '00:53:49', '01:24:13', '01:04:19'] \n",
    "\n",
    "3394 tracks including      357 ghosts ( 10.5%). Event average  10.5%\n",
    "              velo :     2466 from     3791 ( 65.0%,  67.1%)      301 clones ( 12.21%), purity: ( 98.21%,  98.21%),  hitEff: ( 74.98%,  74.94%)\n",
    "              long :      915 from     1070 ( 85.5%,  86.9%)      194 clones ( 21.20%), purity: ( 98.24%,  98.25%),  hitEff: ( 69.00%,  68.80%)\n",
    "         long>5GeV :      611 from      685 ( 89.2%,  89.5%)      145 clones ( 23.73%), purity: ( 98.14%,  98.15%),  hitEff: ( 67.43%,  67.32%)\n",
    "      long_strange :       39 from       51 ( 76.5%,  77.8%)        4 clones ( 10.26%), purity: ( 96.86%,  96.89%),  hitEff: ( 77.72%,  72.79%)\n",
    " long_strange>5GeV :       22 from       27 ( 81.5%,  82.0%)        1 clones (  4.55%), purity: ( 97.02%,  98.04%),  hitEff: ( 78.98%,  75.60%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 10th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 10th sample of minibias dataset (Samples_3412_to_6786_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 2.0, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 1875 mins 23.09 seconds.\n",
    "Average number of iterations per convergence: 78.4 iterations. \n",
    "\n",
    "ID of each event: [484, 500, 331, 523, 988, 596, 720, 517, 78, 130]\n",
    "Number of hits by event: [3532, 5482, 3783, 3865, 5879, 4014, 3927, 4267, 4377, 4168] \n",
    "Number of max_neurons by event: [7744, 19740, 11016, 13870, 17157, 17554, 12317, 11155, 16899, 9310] \n",
    "Hopfield networks runtime by event: ['02:02:11', '06:20:33', '02:00:11', '02:15:49', '04:42:04', '02:09:41', '02:08:36', '02:36:11', '03:06:13', '03:53:49'] \n",
    "\n",
    "4799 tracks including      661 ghosts ( 13.8%). Event average  13.6%\n",
    "              velo :     3273 from     5404 ( 60.6%,  61.0%)      478 clones ( 14.60%), purity: ( 97.94%,  98.00%),  hitEff: ( 69.34%,  68.78%)\n",
    "              long :     1164 from     1479 ( 78.7%,  79.3%)      317 clones ( 27.23%), purity: ( 97.94%,  98.00%),  hitEff: ( 61.57%,  60.44%)\n",
    "         long>5GeV :      735 from      902 ( 81.5%,  82.1%)      209 clones ( 28.44%), purity: ( 97.87%,  97.94%),  hitEff: ( 60.23%,  59.06%)\n",
    "      long_strange :       57 from       83 ( 68.7%,  70.2%)        8 clones ( 14.04%), purity: ( 97.93%,  98.02%),  hitEff: ( 72.42%,  70.29%)\n",
    " long_strange>5GeV :       30 from       44 ( 68.2%,  75.3%)        3 clones ( 10.00%), purity: ( 98.32%,  98.62%),  hitEff: ( 78.94%,  75.95%)\n",
    "        long_fromb :        3 from        4 ( 75.0%,  70.8%)        1 clones ( 33.33%), purity: (100.00%, 100.00%),  hitEff: ( 65.36%,  65.36%)\n",
    "   long_fromb>5GeV :        1 from        1 (100.0%, 100.0%)        1 clones (100.00%), purity: (100.00%, 100.00%),  hitEff: ( 45.00%,  45.00%)\n",
    "____________________\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create lists to store data for each metric\n",
    "reconstruction_efficiency_data = []\n",
    "percentage_clone_data = []\n",
    "purity_data = []\n",
    "hit_efficiency_data = []\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if len(data_lines) == 6:  # Ensure that there are enough lines in the section\n",
    "        reconstruction_efficiency_values = pattern.findall(data_lines[0])\n",
    "        percentage_clone_values = pattern.findall(data_lines[1])\n",
    "        purity_values = pattern.findall(data_lines[2])\n",
    "        hit_efficiency_values = pattern.findall(data_lines[3])\n",
    "        \n",
    "        # Handle cases with additional particle types\n",
    "        particle_types = [\"velo\", \"long\", \"long>5GeV\", \"long_strange\", \"long_strange>5GeV\",\n",
    "                          \"long_fromb\", \"long_fromb>5GeV\"]\n",
    "        particle_values = []\n",
    "        for line in data_lines[4:]:\n",
    "            parts = pattern.findall(line)\n",
    "            if parts:\n",
    "                particle_values.extend(parts)\n",
    "            else:\n",
    "                particle_values.extend([\"N/A\"] * len(particle_types))\n",
    "                break  # Stop processing when no more percentage values are found\n",
    "        \n",
    "        if len(reconstruction_efficiency_values) == 1:\n",
    "            reconstruction_efficiency_data.append([f\"Experiment {i+1}\", *reconstruction_efficiency_values])\n",
    "        if len(percentage_clone_values) == len(particle_types):\n",
    "            percentage_clone_data.append([f\"Experiment {i+1}\", *percentage_clone_values])\n",
    "        if len(purity_values) == len(particle_types):\n",
    "            purity_data.append([f\"Experiment {i+1}\", *purity_values])\n",
    "        if len(hit_efficiency_values) == len(particle_types):\n",
    "            hit_efficiency_data.append([f\"Experiment {i+1}\", *hit_efficiency_values])\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data, columns=['Experiment', 'Value1'])\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data, columns=['Experiment'] + particle_types)\n",
    "purity_df = pd.DataFrame(purity_data, columns=['Experiment'] + particle_types)\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data, columns=['Experiment'] + particle_types)\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "     Experiment Value1\n",
      "0  Experiment 1    8.9\n",
      "1  Experiment 2    9.4\n",
      "2  Experiment 3    9.2\n",
      "3  Experiment 4   10.5\n",
      "4  Experiment 5   13.8\n",
      "\n",
      "Percentage Clone DataFrames:\n",
      "velo\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "long\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "long>5GeV\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "long_strange\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "long_strange>5GeV\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "long_fromb\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n",
      "long_fromb>5GeV\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, velo, long, long>5GeV, long_strange, long_strange>5GeV, long_fromb, long_fromb>5GeV]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 6th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 6th sample of minibias dataset (Samples_1819_to_2119_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1.24, 'T_decay': <function <lambda> at 0x7fbcf1eb55e0>, 'B_decay': <function <lambda> at 0x7fbcf1eb5670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 157 mins 25.64 seconds.\n",
    "Average number of iterations per convergence: 46.65 iterations. \n",
    "\n",
    "ID of each event: [395, 971, 786, 479, 10, 156, 353, 216, 498, 711]\n",
    "Number of hits by event: [1898, 1824, 2089, 1989, 1874, 1907, 2056, 1915, 1988, 2080] \n",
    "Number of max_neurons by event: [2700, 2254, 6000, 2860, 3480, 3968, 2805, 2856, 3190, 4032] \n",
    "Hopfield networks runtime by event: ['00:12:13', '00:11:45', '00:18:36', '00:14:20', '00:12:07', '00:21:40', '00:16:21', '00:14:42', '00:17:35', '00:18:00'] \n",
    "\n",
    "2158 tracks including      191 ghosts (  8.9%). Event average   8.8%\n",
    "              velo :     1586 from     2340 ( 67.8%,  66.6%)      171 clones ( 10.78%), purity: ( 98.19%,  98.18%),  hitEff: ( 77.24%,  77.24%)\n",
    "              long :      585 from      664 ( 88.1%,  86.3%)      109 clones ( 18.63%), purity: ( 98.43%,  98.51%),  hitEff: ( 72.67%,  72.48%)\n",
    "         long>5GeV :      377 from      421 ( 89.5%,  88.7%)       78 clones ( 20.69%), purity: ( 98.16%,  98.28%),  hitEff: ( 71.58%,  71.20%)\n",
    "      long_strange :       31 from       39 ( 79.5%,  76.0%)        5 clones ( 16.13%), purity: ( 97.22%,  98.07%),  hitEff: ( 71.07%,  71.67%)\n",
    " long_strange>5GeV :       18 from       23 ( 78.3%,  91.1%)        2 clones ( 11.11%), purity: ( 96.00%,  96.72%),  hitEff: ( 75.95%,  75.93%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 7th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 7th sample of minibias dataset (Samples_2121_to_2464_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7fbf79017550>, 'B_decay': <function <lambda> at 0x7fbf790175e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 234 mins 43.82 seconds.\n",
    "Average number of iterations per convergence: 50.9 iterations. \n",
    "\n",
    "ID of each event: [383, 95, 547, 217, 299, 721, 430, 105, 496, 1]\n",
    "Number of hits by event: [2366, 2319, 2399, 2418, 2250, 2157, 2123, 2180, 2372, 2422] \n",
    "Number of max_neurons by event: [2970, 3024, 3348, 4032, 3906, 2940, 3480, 3024, 3420, 3306] \n",
    "Hopfield networks runtime by event: ['00:24:05', '00:21:56', '00:27:21', '00:31:19', '00:23:19', '00:18:06', '00:17:00', '00:20:40', '00:23:50', '00:27:02'] \n",
    "\n",
    "2558 tracks including      240 ghosts (  9.4%). Event average   9.4%\n",
    "              velo :     1829 from     2714 ( 67.4%,  67.8%)      239 clones ( 13.07%), purity: ( 98.25%,  98.24%),  hitEff: ( 75.98%,  76.01%)\n",
    "              long :      657 from      728 ( 90.2%,  92.0%)      143 clones ( 21.77%), purity: ( 98.49%,  98.45%),  hitEff: ( 70.41%,  70.44%)\n",
    "         long>5GeV :      417 from      460 ( 90.7%,  91.5%)      101 clones ( 24.22%), purity: ( 98.53%,  98.50%),  hitEff: ( 69.31%,  69.15%)\n",
    "      long_strange :       28 from       34 ( 82.4%,  89.4%)        6 clones ( 21.43%), purity: ( 99.58%,  99.52%),  hitEff: ( 68.61%,  70.57%)\n",
    " long_strange>5GeV :       14 from       17 ( 82.4%,  91.4%)        5 clones ( 35.71%), purity: ( 99.25%,  98.21%),  hitEff: ( 70.74%,  63.83%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 8th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 8th sample of minibias dataset (Samples_2468_to_2853_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7ff8ab0c1550>, 'B_decay': <function <lambda> at 0x7ff8ab0c15e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 420 mins 12.23 seconds.\n",
    "Average number of iterations per convergence: 51.900000000000006 iterations. \n",
    "\n",
    "ID of each event: [387, 357, 642, 694, 413, 186, 277, 723, 388, 438]\n",
    "Number of hits by event: [2621, 2769, 2823, 2478, 2643, 2728, 2623, 2702, 2547, 2523] \n",
    "Number of max_neurons by event: [6474, 5328, 5037, 3660, 7387, 5183, 3780, 5550, 6474, 4356] \n",
    "Hopfield networks runtime by event: ['00:46:43', '00:42:54', '00:46:03', '00:27:54', '01:03:20', '00:46:26', '00:35:52', '00:41:26', '00:38:18', '00:31:12'] \n",
    "\n",
    "2862 tracks including      264 ghosts (  9.2%). Event average   9.2%\n",
    "              velo :     2092 from     3231 ( 64.7%,  64.8%)      279 clones ( 13.34%), purity: ( 98.26%,  98.27%),  hitEff: ( 75.25%,  75.17%)\n",
    "              long :      750 from      879 ( 85.3%,  85.3%)      181 clones ( 24.13%), purity: ( 98.49%,  98.50%),  hitEff: ( 69.07%,  68.54%)\n",
    "         long>5GeV :      528 from      584 ( 90.4%,  89.8%)      147 clones ( 27.84%), purity: ( 98.46%,  98.46%),  hitEff: ( 66.56%,  65.92%)\n",
    "      long_strange :       47 from       57 ( 82.5%,  81.0%)        4 clones (  8.51%), purity: ( 99.65%,  99.56%),  hitEff: ( 83.51%,  80.23%)\n",
    " long_strange>5GeV :       22 from       25 ( 88.0%,  82.4%)        4 clones ( 18.18%), purity: (100.00%, 100.00%),  hitEff: ( 78.81%,  74.02%)\n",
    "        long_fromb :        0 from        1 (  0.0%,   0.0%)        0 clones (  0.00%), purity: (  0.00%,   0.00%),  hitEff: (  0.00%,   0.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 9th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 9th sample of minibias dataset (Samples_2854_to_3405_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 723 mins 9.39 seconds.\n",
    "Average number of iterations per convergence: 56.8 iterations. \n",
    "\n",
    "ID of each event: [504, 869, 326, 797, 836, 727, 594, 753, 280, 607]\n",
    "Number of hits by event: [3304, 3008, 3350, 2957, 2940, 3375, 2879, 2992, 3157, 2951] \n",
    "Number of max_neurons by event: [7656, 5402, 7917, 6806, 5100, 8366, 6776, 5293, 7138, 6560] \n",
    "Hopfield networks runtime by event: ['01:36:38', '01:06:27', '01:26:27', '01:00:10', '01:00:00', '01:36:06', '00:54:56', '00:53:49', '01:24:13', '01:04:19'] \n",
    "\n",
    "3394 tracks including      357 ghosts ( 10.5%). Event average  10.5%\n",
    "              velo :     2466 from     3791 ( 65.0%,  67.1%)      301 clones ( 12.21%), purity: ( 98.21%,  98.21%),  hitEff: ( 74.98%,  74.94%)\n",
    "              long :      915 from     1070 ( 85.5%,  86.9%)      194 clones ( 21.20%), purity: ( 98.24%,  98.25%),  hitEff: ( 69.00%,  68.80%)\n",
    "         long>5GeV :      611 from      685 ( 89.2%,  89.5%)      145 clones ( 23.73%), purity: ( 98.14%,  98.15%),  hitEff: ( 67.43%,  67.32%)\n",
    "      long_strange :       39 from       51 ( 76.5%,  77.8%)        4 clones ( 10.26%), purity: ( 96.86%,  96.89%),  hitEff: ( 77.72%,  72.79%)\n",
    " long_strange>5GeV :       22 from       27 ( 81.5%,  82.0%)        1 clones (  4.55%), purity: ( 97.02%,  98.04%),  hitEff: ( 78.98%,  75.60%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 10th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 10th sample of minibias dataset (Samples_3412_to_6786_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 2.0, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 1875 mins 23.09 seconds.\n",
    "Average number of iterations per convergence: 78.4 iterations. \n",
    "\n",
    "ID of each event: [484, 500, 331, 523, 988, 596, 720, 517, 78, 130]\n",
    "Number of hits by event: [3532, 5482, 3783, 3865, 5879, 4014, 3927, 4267, 4377, 4168] \n",
    "Number of max_neurons by event: [7744, 19740, 11016, 13870, 17157, 17554, 12317, 11155, 16899, 9310] \n",
    "Hopfield networks runtime by event: ['02:02:11', '06:20:33', '02:00:11', '02:15:49', '04:42:04', '02:09:41', '02:08:36', '02:36:11', '03:06:13', '03:53:49'] \n",
    "\n",
    "4799 tracks including      661 ghosts ( 13.8%). Event average  13.6%\n",
    "              velo :     3273 from     5404 ( 60.6%,  61.0%)      478 clones ( 14.60%), purity: ( 97.94%,  98.00%),  hitEff: ( 69.34%,  68.78%)\n",
    "              long :     1164 from     1479 ( 78.7%,  79.3%)      317 clones ( 27.23%), purity: ( 97.94%,  98.00%),  hitEff: ( 61.57%,  60.44%)\n",
    "         long>5GeV :      735 from      902 ( 81.5%,  82.1%)      209 clones ( 28.44%), purity: ( 97.87%,  97.94%),  hitEff: ( 60.23%,  59.06%)\n",
    "      long_strange :       57 from       83 ( 68.7%,  70.2%)        8 clones ( 14.04%), purity: ( 97.93%,  98.02%),  hitEff: ( 72.42%,  70.29%)\n",
    " long_strange>5GeV :       30 from       44 ( 68.2%,  75.3%)        3 clones ( 10.00%), purity: ( 98.32%,  98.62%),  hitEff: ( 78.94%,  75.95%)\n",
    "        long_fromb :        3 from        4 ( 75.0%,  70.8%)        1 clones ( 33.33%), purity: (100.00%, 100.00%),  hitEff: ( 65.36%,  65.36%)\n",
    "   long_fromb>5GeV :        1 from        1 (100.0%, 100.0%)        1 clones (100.00%), purity: (100.00%, 100.00%),  hitEff: ( 45.00%,  45.00%)\n",
    "____________________\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create dictionaries to store data for each metric\n",
    "reconstruction_efficiency_data = []\n",
    "percentage_clone_data = {}\n",
    "purity_data = {}\n",
    "hit_efficiency_data = {}\n",
    "\n",
    "# Define the particle types\n",
    "particle_types = [\"velo\", \"long\", \"long>5GeV\", \"long_strange\", \"long_strange>5GeV\",\n",
    "                  \"long_fromb\", \"long_fromb>5GeV\"]\n",
    "\n",
    "# Initialize the percentage_clone_data and purity_data dictionaries\n",
    "for particle_type in particle_types:\n",
    "    percentage_clone_data[particle_type] = []\n",
    "    purity_data[particle_type] = []\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for i, section in enumerate(sections):\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if len(data_lines) >= 6:  # Ensure that there are enough lines in the section\n",
    "        reconstruction_efficiency_values = pattern.findall(data_lines[0])\n",
    "\n",
    "        if len(reconstruction_efficiency_values) == 1:\n",
    "            reconstruction_efficiency_data.append([f\"Experiment {i+1}\", *reconstruction_efficiency_values])\n",
    "\n",
    "        for j, line in enumerate(data_lines[4:]):\n",
    "            parts = pattern.findall(line)\n",
    "            if parts:\n",
    "                if len(parts) == len(particle_types):\n",
    "                    percentage_clone_data[particle_types[j]].append([f\"Experiment {i+1}\", *parts])\n",
    "                elif len(parts) == len(particle_types) + 2:  # Account for the additional types\n",
    "                    percentage_clone_data[particle_types[j]].append([f\"Experiment {i+1}\", *parts[:-2]])\n",
    "                elif len(parts) == len(particle_types) + 4:\n",
    "                    percentage_clone_data[particle_types[j]].append([f\"Experiment {i+1}\", *parts[:-4]])\n",
    "            else:\n",
    "                percentage_clone_data[particle_types[j]].append([f\"Experiment {i+1}\", \"N/A\"] * len(particle_types))\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data, columns=['Experiment', 'Value1'])\n",
    "\n",
    "percentage_clone_dfs = {}\n",
    "for particle_type in particle_types:\n",
    "    percentage_clone_dfs[particle_type] = pd.DataFrame(percentage_clone_data[particle_type], columns=['Experiment'] + particle_types)\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrames:\")\n",
    "for particle_type in particle_types:\n",
    "    print(particle_type)\n",
    "    print(percentage_clone_dfs[particle_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "Empty DataFrame\n",
      "Columns: [Experiment, Reconstruction Efficiency]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The following 'id_vars' are not present in the DataFrame: ['Experiment']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 170\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data)\n\u001b[1;32m--> 170\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mmelt(id_vars\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mExperiment\u001b[39;49m\u001b[39m\"\u001b[39;49m], value_name\u001b[39m=\u001b[39;49mmetric, var_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mParticle Type\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    172\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m}\u001b[39;00m\u001b[39m DataFrame:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:7367\u001b[0m, in \u001b[0;36mDataFrame.melt\u001b[1;34m(self, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[0;32m   7356\u001b[0m \u001b[39m@Appender\u001b[39m(_shared_docs[\u001b[39m\"\u001b[39m\u001b[39mmelt\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcaller\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mdf.melt(\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mother\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mmelt\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m   7357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmelt\u001b[39m(\n\u001b[0;32m   7358\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   7364\u001b[0m     ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   7365\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m-> 7367\u001b[0m     \u001b[39mreturn\u001b[39;00m melt(\n\u001b[0;32m   7368\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   7369\u001b[0m         id_vars\u001b[39m=\u001b[39;49mid_vars,\n\u001b[0;32m   7370\u001b[0m         value_vars\u001b[39m=\u001b[39;49mvalue_vars,\n\u001b[0;32m   7371\u001b[0m         var_name\u001b[39m=\u001b[39;49mvar_name,\n\u001b[0;32m   7372\u001b[0m         value_name\u001b[39m=\u001b[39;49mvalue_name,\n\u001b[0;32m   7373\u001b[0m         col_level\u001b[39m=\u001b[39;49mcol_level,\n\u001b[0;32m   7374\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m   7375\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\aurel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\melt.py:64\u001b[0m, in \u001b[0;36mmelt\u001b[1;34m(frame, id_vars, value_vars, var_name, value_name, col_level, ignore_index)\u001b[0m\n\u001b[0;32m     62\u001b[0m         missing \u001b[39m=\u001b[39m Index(com\u001b[39m.\u001b[39mflatten(id_vars))\u001b[39m.\u001b[39mdifference(cols)\n\u001b[0;32m     63\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m missing\u001b[39m.\u001b[39mempty:\n\u001b[1;32m---> 64\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mThe following \u001b[39m\u001b[39m'\u001b[39m\u001b[39mid_vars\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are not present \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the DataFrame: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(missing)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m             )\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     id_vars \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The following 'id_vars' are not present in the DataFrame: ['Experiment']\""
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Your data\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 6th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 6th sample of minibias dataset (Samples_1819_to_2119_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1.24, 'T_decay': <function <lambda> at 0x7fbcf1eb55e0>, 'B_decay': <function <lambda> at 0x7fbcf1eb5670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 157 mins 25.64 seconds.\n",
    "Average number of iterations per convergence: 46.65 iterations. \n",
    "\n",
    "ID of each event: [395, 971, 786, 479, 10, 156, 353, 216, 498, 711]\n",
    "Number of hits by event: [1898, 1824, 2089, 1989, 1874, 1907, 2056, 1915, 1988, 2080] \n",
    "Number of max_neurons by event: [2700, 2254, 6000, 2860, 3480, 3968, 2805, 2856, 3190, 4032] \n",
    "Hopfield networks runtime by event: ['00:12:13', '00:11:45', '00:18:36', '00:14:20', '00:12:07', '00:21:40', '00:16:21', '00:14:42', '00:17:35', '00:18:00'] \n",
    "\n",
    "2158 tracks including      191 ghosts (  8.9%). Event average   8.8%\n",
    "              velo :     1586 from     2340 ( 67.8%,  66.6%)      171 clones ( 10.78%), purity: ( 98.19%,  98.18%),  hitEff: ( 77.24%,  77.24%)\n",
    "              long :      585 from      664 ( 88.1%,  86.3%)      109 clones ( 18.63%), purity: ( 98.43%,  98.51%),  hitEff: ( 72.67%,  72.48%)\n",
    "         long>5GeV :      377 from      421 ( 89.5%,  88.7%)       78 clones ( 20.69%), purity: ( 98.16%,  98.28%),  hitEff: ( 71.58%,  71.20%)\n",
    "      long_strange :       31 from       39 ( 79.5%,  76.0%)        5 clones ( 16.13%), purity: ( 97.22%,  98.07%),  hitEff: ( 71.07%,  71.67%)\n",
    " long_strange>5GeV :       18 from       23 ( 78.3%,  91.1%)        2 clones ( 11.11%), purity: ( 96.00%,  96.72%),  hitEff: ( 75.95%,  75.93%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 7th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 7th sample of minibias dataset (Samples_2121_to_2464_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7fbf79017550>, 'B_decay': <function <lambda> at 0x7fbf790175e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 234 mins 43.82 seconds.\n",
    "Average number of iterations per convergence: 50.9 iterations. \n",
    "\n",
    "ID of each event: [383, 95, 547, 217, 299, 721, 430, 105, 496, 1]\n",
    "Number of hits by event: [2366, 2319, 2399, 2418, 2250, 2157, 2123, 2180, 2372, 2422] \n",
    "Number of max_neurons by event: [2970, 3024, 3348, 4032, 3906, 2940, 3480, 3024, 3420, 3306] \n",
    "Hopfield networks runtime by event: ['00:24:05', '00:21:56', '00:27:21', '00:31:19', '00:23:19', '00:18:06', '00:17:00', '00:20:40', '00:23:50', '00:27:02'] \n",
    "\n",
    "2558 tracks including      240 ghosts (  9.4%). Event average   9.4%\n",
    "              velo :     1829 from     2714 ( 67.4%,  67.8%)      239 clones ( 13.07%), purity: ( 98.25%,  98.24%),  hitEff: ( 75.98%,  76.01%)\n",
    "              long :      657 from      728 ( 90.2%,  92.0%)      143 clones ( 21.77%), purity: ( 98.49%,  98.45%),  hitEff: ( 70.41%,  70.44%)\n",
    "         long>5GeV :      417 from      460 ( 90.7%,  91.5%)      101 clones ( 24.22%), purity: ( 98.53%,  98.50%),  hitEff: ( 69.31%,  69.15%)\n",
    "      long_strange :       28 from       34 ( 82.4%,  89.4%)        6 clones ( 21.43%), purity: ( 99.58%,  99.52%),  hitEff: ( 68.61%,  70.57%)\n",
    " long_strange>5GeV :       14 from       17 ( 82.4%,  91.4%)        5 clones ( 35.71%), purity: ( 99.25%,  98.21%),  hitEff: ( 70.74%,  63.83%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 8th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 8th sample of minibias dataset (Samples_2468_to_2853_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7ff8ab0c1550>, 'B_decay': <function <lambda> at 0x7ff8ab0c15e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 420 mins 12.23 seconds.\n",
    "Average number of iterations per convergence: 51.900000000000006 iterations. \n",
    "\n",
    "ID of each event: [387, 357, 642, 694, 413, 186, 277, 723, 388, 438]\n",
    "Number of hits by event: [2621, 2769, 2823, 2478, 2643, 2728, 2623, 2702, 2547, 2523] \n",
    "Number of max_neurons by event: [6474, 5328, 5037, 3660, 7387, 5183, 3780, 5550, 6474, 4356] \n",
    "Hopfield networks runtime by event: ['00:46:43', '00:42:54', '00:46:03', '00:27:54', '01:03:20', '00:46:26', '00:35:52', '00:41:26', '00:38:18', '00:31:12'] \n",
    "\n",
    "2862 tracks including      264 ghosts (  9.2%). Event average   9.2%\n",
    "              velo :     2092 from     3231 ( 64.7%,  64.8%)      279 clones ( 13.34%), purity: ( 98.26%,  98.27%),  hitEff: ( 75.25%,  75.17%)\n",
    "              long :      750 from      879 ( 85.3%,  85.3%)      181 clones ( 24.13%), purity: ( 98.49%,  98.50%),  hitEff: ( 69.07%,  68.54%)\n",
    "         long>5GeV :      528 from      584 ( 90.4%,  89.8%)      147 clones ( 27.84%), purity: ( 98.46%,  98.46%),  hitEff: ( 66.56%,  65.92%)\n",
    "      long_strange :       47 from       57 ( 82.5%,  81.0%)        4 clones (  8.51%), purity: ( 99.65%,  99.56%),  hitEff: ( 83.51%,  80.23%)\n",
    " long_strange>5GeV :       22 from       25 ( 88.0%,  82.4%)        4 clones ( 18.18%), purity: (100.00%, 100.00%),  hitEff: ( 78.81%,  74.02%)\n",
    "        long_fromb :        0 from        1 (  0.0%,   0.0%)        0 clones (  0.00%), purity: (  0.00%,   0.00%),  hitEff: (  0.00%,   0.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 9th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 9th sample of minibias dataset (Samples_2854_to_3405_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 723 mins 9.39 seconds.\n",
    "Average number of iterations per convergence: 56.8 iterations. \n",
    "\n",
    "ID of each event: [504, 869, 326, 797, 836, 727, 594, 753, 280, 607]\n",
    "Number of hits by event: [3304, 3008, 3350, 2957, 2940, 3375, 2879, 2992, 3157, 2951] \n",
    "Number of max_neurons by event: [7656, 5402, 7917, 6806, 5100, 8366, 6776, 5293, 7138, 6560] \n",
    "Hopfield networks runtime by event: ['01:36:38', '01:06:27', '01:26:27', '01:00:10', '01:00:00', '01:36:06', '00:54:56', '00:53:49', '01:24:13', '01:04:19'] \n",
    "\n",
    "3394 tracks including      357 ghosts ( 10.5%). Event average  10.5%\n",
    "              velo :     2466 from     3791 ( 65.0%,  67.1%)      301 clones ( 12.21%), purity: ( 98.21%,  98.21%),  hitEff: ( 74.98%,  74.94%)\n",
    "              long :      915 from     1070 ( 85.5%,  86.9%)      194 clones ( 21.20%), purity: ( 98.24%,  98.25%),  hitEff: ( 69.00%,  68.80%)\n",
    "         long>5GeV :      611 from      685 ( 89.2%,  89.5%)      145 clones ( 23.73%), purity: ( 98.14%,  98.15%),  hitEff: ( 67.43%,  67.32%)\n",
    "      long_strange :       39 from       51 ( 76.5%,  77.8%)        4 clones ( 10.26%), purity: ( 96.86%,  96.89%),  hitEff: ( 77.72%,  72.79%)\n",
    " long_strange>5GeV :       22 from       27 ( 81.5%,  82.0%)        1 clones (  4.55%), purity: ( 97.02%,  98.04%),  hitEff: ( 78.98%,  75.60%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 10th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 10th sample of minibias dataset (Samples_3412_to_6786_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 2.0, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 1875 mins 23.09 seconds.\n",
    "Average number of iterations per convergence: 78.4 iterations. \n",
    "\n",
    "ID of each event: [484, 500, 331, 523, 988, 596, 720, 517, 78, 130]\n",
    "Number of hits by event: [3532, 5482, 3783, 3865, 5879, 4014, 3927, 4267, 4377, 4168] \n",
    "Number of max_neurons by event: [7744, 19740, 11016, 13870, 17157, 17554, 12317, 11155, 16899, 9310] \n",
    "Hopfield networks runtime by event: ['02:02:11', '06:20:33', '02:00:11', '02:15:49', '04:42:04', '02:09:41', '02:08:36', '02:36:11', '03:06:13', '03:53:49'] \n",
    "\n",
    "4799 tracks including      661 ghosts ( 13.8%). Event average  13.6%\n",
    "              velo :     3273 from     5404 ( 60.6%,  61.0%)      478 clones ( 14.60%), purity: ( 97.94%,  98.00%),  hitEff: ( 69.34%,  68.78%)\n",
    "              long :     1164 from     1479 ( 78.7%,  79.3%)      317 clones ( 27.23%), purity: ( 97.94%,  98.00%),  hitEff: ( 61.57%,  60.44%)\n",
    "         long>5GeV :      735 from      902 ( 81.5%,  82.1%)      209 clones ( 28.44%), purity: ( 97.87%,  97.94%),  hitEff: ( 60.23%,  59.06%)\n",
    "      long_strange :       57 from       83 ( 68.7%,  70.2%)        8 clones ( 14.04%), purity: ( 97.93%,  98.02%),  hitEff: ( 72.42%,  70.29%)\n",
    " long_strange>5GeV :       30 from       44 ( 68.2%,  75.3%)        3 clones ( 10.00%), purity: ( 98.32%,  98.62%),  hitEff: ( 78.94%,  75.95%)\n",
    "        long_fromb :        3 from        4 ( 75.0%,  70.8%)        1 clones ( 33.33%), purity: (100.00%, 100.00%),  hitEff: ( 65.36%,  65.36%)\n",
    "   long_fromb>5GeV :        1 from        1 (100.0%, 100.0%)        1 clones (100.00%), purity: (100.00%, 100.00%),  hitEff: ( 45.00%,  45.00%)\n",
    "____________________\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expressions to match the data lines\n",
    "percentage_pattern = re.compile(r\"(\\d+\\.\\d+)%\")\n",
    "clones_pattern = re.compile(r\"(\\d+\\.\\d+)\\)\")\n",
    "purity_hit_pattern = re.compile(r\"(\\d+\\.\\d+)\\), purity: \\((\\d+\\.\\d+)%\")\n",
    "reconstruction_efficiency_pattern = re.compile(r\"(\\d+\\.\\d+)\")\n",
    "\n",
    "# Define particle types\n",
    "particle_types = [\"velo\", \"long\", \"long>5GeV\", \"long_strange\", \"long_strange>5GeV\",\n",
    "                  \"long_fromb\", \"long_fromb>5GeV\"]\n",
    "\n",
    "# Initialize dictionaries to store data for each metric\n",
    "metrics_data = {\n",
    "    \"Reconstruction Efficiency\": [],\n",
    "    \"Percentage Clone\": {ptype: [] for ptype in particle_types},\n",
    "    \"Purity\": {ptype: [] for ptype in particle_types},\n",
    "    \"Hit Efficiency\": {ptype: [] for ptype in particle_types}\n",
    "}\n",
    "\n",
    "# Loop through each section\n",
    "for i, section in enumerate(sections):\n",
    "    lines = section.strip().split('\\n')\n",
    "    experiment = f\"Experiment {i+1}\"\n",
    "\n",
    "    # Get the reconstruction efficiency value\n",
    "    recon_efficiency_match = reconstruction_efficiency_pattern.search(lines[0])\n",
    "    if recon_efficiency_match:\n",
    "        recon_efficiency = float(recon_efficiency_match.group(1))\n",
    "        metrics_data[\"Reconstruction Efficiency\"].append((experiment, recon_efficiency))\n",
    "\n",
    "    # Loop through particle types and corresponding data lines\n",
    "    for particle_type, data_line in zip(particle_types, lines[1:]):\n",
    "        percentage_match = percentage_pattern.findall(data_line)\n",
    "        clones_match = clones_pattern.findall(data_line)\n",
    "        purity_hit_match = purity_hit_pattern.findall(data_line)\n",
    "\n",
    "        if percentage_match:\n",
    "            metrics_data[\"Percentage Clone\"][particle_type].append((experiment, float(percentage_match[0])))\n",
    "\n",
    "        if clones_match:\n",
    "            metrics_data[\"Purity\"][particle_type].append((experiment, float(clones_match[0])))\n",
    "\n",
    "        if purity_hit_match:\n",
    "            metrics_data[\"Hit Efficiency\"][particle_type].append((experiment, float(purity_hit_match[0])))\n",
    "\n",
    "# Create DataFrames for each metric and save to separate Excel files\n",
    "for metric, data in metrics_data.items():\n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data, columns=[\"Experiment\", metric])\n",
    "    else:\n",
    "        df = pd.DataFrame(data)\n",
    "        df = df.melt(id_vars=[\"Experiment\"], value_name=metric, var_name=\"Particle Type\")\n",
    "\n",
    "    print(f\"{metric} DataFrame:\")\n",
    "    print(df)\n",
    "    df.to_excel(f\"experiment_{metric.lower().replace(' ', '_')}.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39m# Extract and process data lines for each metric\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m i, metric_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(metrics_data\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m--> 141\u001b[0m     values \u001b[39m=\u001b[39m value_pattern\u001b[39m.\u001b[39mfindall(lines[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m    142\u001b[0m     percentages \u001b[39m=\u001b[39m percentage_pattern\u001b[39m.\u001b[39mfindall(lines[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m values:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Your data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 6th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 6th sample of minibias dataset (Samples_1819_to_2119_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1.24, 'T_decay': <function <lambda> at 0x7fbcf1eb55e0>, 'B_decay': <function <lambda> at 0x7fbcf1eb5670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 157 mins 25.64 seconds.\n",
    "Average number of iterations per convergence: 46.65 iterations. \n",
    "\n",
    "ID of each event: [395, 971, 786, 479, 10, 156, 353, 216, 498, 711]\n",
    "Number of hits by event: [1898, 1824, 2089, 1989, 1874, 1907, 2056, 1915, 1988, 2080] \n",
    "Number of max_neurons by event: [2700, 2254, 6000, 2860, 3480, 3968, 2805, 2856, 3190, 4032] \n",
    "Hopfield networks runtime by event: ['00:12:13', '00:11:45', '00:18:36', '00:14:20', '00:12:07', '00:21:40', '00:16:21', '00:14:42', '00:17:35', '00:18:00'] \n",
    "\n",
    "2158 tracks including      191 ghosts (  8.9%). Event average   8.8%\n",
    "              velo :     1586 from     2340 ( 67.8%,  66.6%)      171 clones ( 10.78%), purity: ( 98.19%,  98.18%),  hitEff: ( 77.24%,  77.24%)\n",
    "              long :      585 from      664 ( 88.1%,  86.3%)      109 clones ( 18.63%), purity: ( 98.43%,  98.51%),  hitEff: ( 72.67%,  72.48%)\n",
    "         long>5GeV :      377 from      421 ( 89.5%,  88.7%)       78 clones ( 20.69%), purity: ( 98.16%,  98.28%),  hitEff: ( 71.58%,  71.20%)\n",
    "      long_strange :       31 from       39 ( 79.5%,  76.0%)        5 clones ( 16.13%), purity: ( 97.22%,  98.07%),  hitEff: ( 71.07%,  71.67%)\n",
    " long_strange>5GeV :       18 from       23 ( 78.3%,  91.1%)        2 clones ( 11.11%), purity: ( 96.00%,  96.72%),  hitEff: ( 75.95%,  75.93%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 7th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 7th sample of minibias dataset (Samples_2121_to_2464_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7fbf79017550>, 'B_decay': <function <lambda> at 0x7fbf790175e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 234 mins 43.82 seconds.\n",
    "Average number of iterations per convergence: 50.9 iterations. \n",
    "\n",
    "ID of each event: [383, 95, 547, 217, 299, 721, 430, 105, 496, 1]\n",
    "Number of hits by event: [2366, 2319, 2399, 2418, 2250, 2157, 2123, 2180, 2372, 2422] \n",
    "Number of max_neurons by event: [2970, 3024, 3348, 4032, 3906, 2940, 3480, 3024, 3420, 3306] \n",
    "Hopfield networks runtime by event: ['00:24:05', '00:21:56', '00:27:21', '00:31:19', '00:23:19', '00:18:06', '00:17:00', '00:20:40', '00:23:50', '00:27:02'] \n",
    "\n",
    "2558 tracks including      240 ghosts (  9.4%). Event average   9.4%\n",
    "              velo :     1829 from     2714 ( 67.4%,  67.8%)      239 clones ( 13.07%), purity: ( 98.25%,  98.24%),  hitEff: ( 75.98%,  76.01%)\n",
    "              long :      657 from      728 ( 90.2%,  92.0%)      143 clones ( 21.77%), purity: ( 98.49%,  98.45%),  hitEff: ( 70.41%,  70.44%)\n",
    "         long>5GeV :      417 from      460 ( 90.7%,  91.5%)      101 clones ( 24.22%), purity: ( 98.53%,  98.50%),  hitEff: ( 69.31%,  69.15%)\n",
    "      long_strange :       28 from       34 ( 82.4%,  89.4%)        6 clones ( 21.43%), purity: ( 99.58%,  99.52%),  hitEff: ( 68.61%,  70.57%)\n",
    " long_strange>5GeV :       14 from       17 ( 82.4%,  91.4%)        5 clones ( 35.71%), purity: ( 99.25%,  98.21%),  hitEff: ( 70.74%,  63.83%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 8th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 8th sample of minibias dataset (Samples_2468_to_2853_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7ff8ab0c1550>, 'B_decay': <function <lambda> at 0x7ff8ab0c15e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 420 mins 12.23 seconds.\n",
    "Average number of iterations per convergence: 51.900000000000006 iterations. \n",
    "\n",
    "ID of each event: [387, 357, 642, 694, 413, 186, 277, 723, 388, 438]\n",
    "Number of hits by event: [2621, 2769, 2823, 2478, 2643, 2728, 2623, 2702, 2547, 2523] \n",
    "Number of max_neurons by event: [6474, 5328, 5037, 3660, 7387, 5183, 3780, 5550, 6474, 4356] \n",
    "Hopfield networks runtime by event: ['00:46:43', '00:42:54', '00:46:03', '00:27:54', '01:03:20', '00:46:26', '00:35:52', '00:41:26', '00:38:18', '00:31:12'] \n",
    "\n",
    "2862 tracks including      264 ghosts (  9.2%). Event average   9.2%\n",
    "              velo :     2092 from     3231 ( 64.7%,  64.8%)      279 clones ( 13.34%), purity: ( 98.26%,  98.27%),  hitEff: ( 75.25%,  75.17%)\n",
    "              long :      750 from      879 ( 85.3%,  85.3%)      181 clones ( 24.13%), purity: ( 98.49%,  98.50%),  hitEff: ( 69.07%,  68.54%)\n",
    "         long>5GeV :      528 from      584 ( 90.4%,  89.8%)      147 clones ( 27.84%), purity: ( 98.46%,  98.46%),  hitEff: ( 66.56%,  65.92%)\n",
    "      long_strange :       47 from       57 ( 82.5%,  81.0%)        4 clones (  8.51%), purity: ( 99.65%,  99.56%),  hitEff: ( 83.51%,  80.23%)\n",
    " long_strange>5GeV :       22 from       25 ( 88.0%,  82.4%)        4 clones ( 18.18%), purity: (100.00%, 100.00%),  hitEff: ( 78.81%,  74.02%)\n",
    "        long_fromb :        0 from        1 (  0.0%,   0.0%)        0 clones (  0.00%), purity: (  0.00%,   0.00%),  hitEff: (  0.00%,   0.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 9th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 9th sample of minibias dataset (Samples_2854_to_3405_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 723 mins 9.39 seconds.\n",
    "Average number of iterations per convergence: 56.8 iterations. \n",
    "\n",
    "ID of each event: [504, 869, 326, 797, 836, 727, 594, 753, 280, 607]\n",
    "Number of hits by event: [3304, 3008, 3350, 2957, 2940, 3375, 2879, 2992, 3157, 2951] \n",
    "Number of max_neurons by event: [7656, 5402, 7917, 6806, 5100, 8366, 6776, 5293, 7138, 6560] \n",
    "Hopfield networks runtime by event: ['01:36:38', '01:06:27', '01:26:27', '01:00:10', '01:00:00', '01:36:06', '00:54:56', '00:53:49', '01:24:13', '01:04:19'] \n",
    "\n",
    "3394 tracks including      357 ghosts ( 10.5%). Event average  10.5%\n",
    "              velo :     2466 from     3791 ( 65.0%,  67.1%)      301 clones ( 12.21%), purity: ( 98.21%,  98.21%),  hitEff: ( 74.98%,  74.94%)\n",
    "              long :      915 from     1070 ( 85.5%,  86.9%)      194 clones ( 21.20%), purity: ( 98.24%,  98.25%),  hitEff: ( 69.00%,  68.80%)\n",
    "         long>5GeV :      611 from      685 ( 89.2%,  89.5%)      145 clones ( 23.73%), purity: ( 98.14%,  98.15%),  hitEff: ( 67.43%,  67.32%)\n",
    "      long_strange :       39 from       51 ( 76.5%,  77.8%)        4 clones ( 10.26%), purity: ( 96.86%,  96.89%),  hitEff: ( 77.72%,  72.79%)\n",
    " long_strange>5GeV :       22 from       27 ( 81.5%,  82.0%)        1 clones (  4.55%), purity: ( 97.02%,  98.04%),  hitEff: ( 78.98%,  75.60%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 10th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 10th sample of minibias dataset (Samples_3412_to_6786_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 2.0, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 1875 mins 23.09 seconds.\n",
    "Average number of iterations per convergence: 78.4 iterations. \n",
    "\n",
    "ID of each event: [484, 500, 331, 523, 988, 596, 720, 517, 78, 130]\n",
    "Number of hits by event: [3532, 5482, 3783, 3865, 5879, 4014, 3927, 4267, 4377, 4168] \n",
    "Number of max_neurons by event: [7744, 19740, 11016, 13870, 17157, 17554, 12317, 11155, 16899, 9310] \n",
    "Hopfield networks runtime by event: ['02:02:11', '06:20:33', '02:00:11', '02:15:49', '04:42:04', '02:09:41', '02:08:36', '02:36:11', '03:06:13', '03:53:49'] \n",
    "\n",
    "4799 tracks including      661 ghosts ( 13.8%). Event average  13.6%\n",
    "              velo :     3273 from     5404 ( 60.6%,  61.0%)      478 clones ( 14.60%), purity: ( 97.94%,  98.00%),  hitEff: ( 69.34%,  68.78%)\n",
    "              long :     1164 from     1479 ( 78.7%,  79.3%)      317 clones ( 27.23%), purity: ( 97.94%,  98.00%),  hitEff: ( 61.57%,  60.44%)\n",
    "         long>5GeV :      735 from      902 ( 81.5%,  82.1%)      209 clones ( 28.44%), purity: ( 97.87%,  97.94%),  hitEff: ( 60.23%,  59.06%)\n",
    "      long_strange :       57 from       83 ( 68.7%,  70.2%)        8 clones ( 14.04%), purity: ( 97.93%,  98.02%),  hitEff: ( 72.42%,  70.29%)\n",
    " long_strange>5GeV :       30 from       44 ( 68.2%,  75.3%)        3 clones ( 10.00%), purity: ( 98.32%,  98.62%),  hitEff: ( 78.94%,  75.95%)\n",
    "        long_fromb :        3 from        4 ( 75.0%,  70.8%)        1 clones ( 33.33%), purity: (100.00%, 100.00%),  hitEff: ( 65.36%,  65.36%)\n",
    "   long_fromb>5GeV :        1 from        1 (100.0%, 100.0%)        1 clones (100.00%), purity: (100.00%, 100.00%),  hitEff: ( 45.00%,  45.00%)\n",
    "____________________\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression patterns for metrics\n",
    "percentage_pattern = re.compile(r\"(\\d+\\.\\d+)%\")\n",
    "value_pattern = re.compile(r\"(\\d+\\.\\d+)\\s*(?:\\(|$)\")\n",
    "\n",
    "# Initialize dictionaries to store data for each metric\n",
    "metrics_data = {\n",
    "    \"Reconstruction Efficiency\": [],\n",
    "    \"Percentage Clone\": [],\n",
    "    \"Purity\": [],\n",
    "    \"Hit Efficiency\": []\n",
    "}\n",
    "\n",
    "# Process each section\n",
    "for section in sections:\n",
    "    lines = section.strip().split(\"\\n\")\n",
    "    \n",
    "    # Extract the experiment number from the first line\n",
    "    experiment_num = lines[0].strip()\n",
    "    \n",
    "    # Extract and process data lines for each metric\n",
    "    for i, metric_name in enumerate(metrics_data.keys()):\n",
    "        values = value_pattern.findall(lines[i+1])\n",
    "        percentages = percentage_pattern.findall(lines[i+1])\n",
    "        \n",
    "        if values:\n",
    "            values = [float(value) for value in values]\n",
    "        else:\n",
    "            values = [\"N/A\"] * len(percentages)\n",
    "        \n",
    "        metrics_data[metric_name].append([experiment_num, *percentages, *values])\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "dfs = {metric_name: pd.DataFrame(data, columns=[\"Experiment\"] + [f\"Value{i}\" for i in range(1, len(data[0]))])\n",
    "       for metric_name, data in metrics_data.items()}\n",
    "\n",
    "# Save DataFrames to Excel files\n",
    "for metric_name, df in dfs.items():\n",
    "    if not df.empty:\n",
    "        excel_path = f\"results/excel/{metric_name.lower().replace(' ', '_')}.xlsx\"\n",
    "        df.to_excel(excel_path, index=False)\n",
    "        print(f\"Excel file saved: {excel_path}\")\n",
    "    else:\n",
    "        print(f\"DataFrame is empty for {metric_name}, skipping...\")\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "for metric_name, df in dfs.items():\n",
    "    print(f\"\\n{metric_name} DataFrame:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "     Experiment Value1\n",
      "0  Experiment 1    8.9\n",
      "1  Experiment 1    9.4\n",
      "2  Experiment 1   10.5\n",
      "\n",
      "Percentage Clone DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 1   66.6  10.78  98.18  77.24\n",
      "1  Experiment 1   67.8  13.07  98.24  76.01\n",
      "2  Experiment 1   67.1  12.21  98.21  74.94\n",
      "\n",
      "Purity DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 1   86.3  18.63  98.51  72.48\n",
      "1  Experiment 1   92.0  21.77  98.45  70.44\n",
      "2  Experiment 1   86.9  21.20  98.25  68.80\n",
      "\n",
      "Hit Efficiency DataFrame:\n",
      "     Experiment Value2 Value3 Value4 Value5\n",
      "0  Experiment 1   88.7  20.69  98.28  71.20\n",
      "1  Experiment 1   91.5  24.22  98.50  69.15\n",
      "2  Experiment 1   89.5  23.73  98.15  67.32\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 6th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 6th sample of minibias dataset (Samples_1819_to_2119_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1.24, 'T_decay': <function <lambda> at 0x7fbcf1eb55e0>, 'B_decay': <function <lambda> at 0x7fbcf1eb5670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 157 mins 25.64 seconds.\n",
    "Average number of iterations per convergence: 46.65 iterations. \n",
    "\n",
    "ID of each event: [395, 971, 786, 479, 10, 156, 353, 216, 498, 711]\n",
    "Number of hits by event: [1898, 1824, 2089, 1989, 1874, 1907, 2056, 1915, 1988, 2080] \n",
    "Number of max_neurons by event: [2700, 2254, 6000, 2860, 3480, 3968, 2805, 2856, 3190, 4032] \n",
    "Hopfield networks runtime by event: ['00:12:13', '00:11:45', '00:18:36', '00:14:20', '00:12:07', '00:21:40', '00:16:21', '00:14:42', '00:17:35', '00:18:00'] \n",
    "\n",
    "2158 tracks including      191 ghosts (  8.9%). Event average   8.8%\n",
    "              velo :     1586 from     2340 ( 67.8%,  66.6%)      171 clones ( 10.78%), purity: ( 98.19%,  98.18%),  hitEff: ( 77.24%,  77.24%)\n",
    "              long :      585 from      664 ( 88.1%,  86.3%)      109 clones ( 18.63%), purity: ( 98.43%,  98.51%),  hitEff: ( 72.67%,  72.48%)\n",
    "         long>5GeV :      377 from      421 ( 89.5%,  88.7%)       78 clones ( 20.69%), purity: ( 98.16%,  98.28%),  hitEff: ( 71.58%,  71.20%)\n",
    "      long_strange :       31 from       39 ( 79.5%,  76.0%)        5 clones ( 16.13%), purity: ( 97.22%,  98.07%),  hitEff: ( 71.07%,  71.67%)\n",
    " long_strange>5GeV :       18 from       23 ( 78.3%,  91.1%)        2 clones ( 11.11%), purity: ( 96.00%,  96.72%),  hitEff: ( 75.95%,  75.93%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 7th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 7th sample of minibias dataset (Samples_2121_to_2464_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7fbf79017550>, 'B_decay': <function <lambda> at 0x7fbf790175e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 234 mins 43.82 seconds.\n",
    "Average number of iterations per convergence: 50.9 iterations. \n",
    "\n",
    "ID of each event: [383, 95, 547, 217, 299, 721, 430, 105, 496, 1]\n",
    "Number of hits by event: [2366, 2319, 2399, 2418, 2250, 2157, 2123, 2180, 2372, 2422] \n",
    "Number of max_neurons by event: [2970, 3024, 3348, 4032, 3906, 2940, 3480, 3024, 3420, 3306] \n",
    "Hopfield networks runtime by event: ['00:24:05', '00:21:56', '00:27:21', '00:31:19', '00:23:19', '00:18:06', '00:17:00', '00:20:40', '00:23:50', '00:27:02'] \n",
    "\n",
    "2558 tracks including      240 ghosts (  9.4%). Event average   9.4%\n",
    "              velo :     1829 from     2714 ( 67.4%,  67.8%)      239 clones ( 13.07%), purity: ( 98.25%,  98.24%),  hitEff: ( 75.98%,  76.01%)\n",
    "              long :      657 from      728 ( 90.2%,  92.0%)      143 clones ( 21.77%), purity: ( 98.49%,  98.45%),  hitEff: ( 70.41%,  70.44%)\n",
    "         long>5GeV :      417 from      460 ( 90.7%,  91.5%)      101 clones ( 24.22%), purity: ( 98.53%,  98.50%),  hitEff: ( 69.31%,  69.15%)\n",
    "      long_strange :       28 from       34 ( 82.4%,  89.4%)        6 clones ( 21.43%), purity: ( 99.58%,  99.52%),  hitEff: ( 68.61%,  70.57%)\n",
    " long_strange>5GeV :       14 from       17 ( 82.4%,  91.4%)        5 clones ( 35.71%), purity: ( 99.25%,  98.21%),  hitEff: ( 70.74%,  63.83%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 8th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 8th sample of minibias dataset (Samples_2468_to_2853_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7ff8ab0c1550>, 'B_decay': <function <lambda> at 0x7ff8ab0c15e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 420 mins 12.23 seconds.\n",
    "Average number of iterations per convergence: 51.900000000000006 iterations. \n",
    "\n",
    "ID of each event: [387, 357, 642, 694, 413, 186, 277, 723, 388, 438]\n",
    "Number of hits by event: [2621, 2769, 2823, 2478, 2643, 2728, 2623, 2702, 2547, 2523] \n",
    "Number of max_neurons by event: [6474, 5328, 5037, 3660, 7387, 5183, 3780, 5550, 6474, 4356] \n",
    "Hopfield networks runtime by event: ['00:46:43', '00:42:54', '00:46:03', '00:27:54', '01:03:20', '00:46:26', '00:35:52', '00:41:26', '00:38:18', '00:31:12'] \n",
    "\n",
    "2862 tracks including      264 ghosts (  9.2%). Event average   9.2%\n",
    "              velo :     2092 from     3231 ( 64.7%,  64.8%)      279 clones ( 13.34%), purity: ( 98.26%,  98.27%),  hitEff: ( 75.25%,  75.17%)\n",
    "              long :      750 from      879 ( 85.3%,  85.3%)      181 clones ( 24.13%), purity: ( 98.49%,  98.50%),  hitEff: ( 69.07%,  68.54%)\n",
    "         long>5GeV :      528 from      584 ( 90.4%,  89.8%)      147 clones ( 27.84%), purity: ( 98.46%,  98.46%),  hitEff: ( 66.56%,  65.92%)\n",
    "      long_strange :       47 from       57 ( 82.5%,  81.0%)        4 clones (  8.51%), purity: ( 99.65%,  99.56%),  hitEff: ( 83.51%,  80.23%)\n",
    " long_strange>5GeV :       22 from       25 ( 88.0%,  82.4%)        4 clones ( 18.18%), purity: (100.00%, 100.00%),  hitEff: ( 78.81%,  74.02%)\n",
    "        long_fromb :        0 from        1 (  0.0%,   0.0%)        0 clones (  0.00%), purity: (  0.00%,   0.00%),  hitEff: (  0.00%,   0.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 9th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 9th sample of minibias dataset (Samples_2854_to_3405_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 723 mins 9.39 seconds.\n",
    "Average number of iterations per convergence: 56.8 iterations. \n",
    "\n",
    "ID of each event: [504, 869, 326, 797, 836, 727, 594, 753, 280, 607]\n",
    "Number of hits by event: [3304, 3008, 3350, 2957, 2940, 3375, 2879, 2992, 3157, 2951] \n",
    "Number of max_neurons by event: [7656, 5402, 7917, 6806, 5100, 8366, 6776, 5293, 7138, 6560] \n",
    "Hopfield networks runtime by event: ['01:36:38', '01:06:27', '01:26:27', '01:00:10', '01:00:00', '01:36:06', '00:54:56', '00:53:49', '01:24:13', '01:04:19'] \n",
    "\n",
    "3394 tracks including      357 ghosts ( 10.5%). Event average  10.5%\n",
    "              velo :     2466 from     3791 ( 65.0%,  67.1%)      301 clones ( 12.21%), purity: ( 98.21%,  98.21%),  hitEff: ( 74.98%,  74.94%)\n",
    "              long :      915 from     1070 ( 85.5%,  86.9%)      194 clones ( 21.20%), purity: ( 98.24%,  98.25%),  hitEff: ( 69.00%,  68.80%)\n",
    "         long>5GeV :      611 from      685 ( 89.2%,  89.5%)      145 clones ( 23.73%), purity: ( 98.14%,  98.15%),  hitEff: ( 67.43%,  67.32%)\n",
    "      long_strange :       39 from       51 ( 76.5%,  77.8%)        4 clones ( 10.26%), purity: ( 96.86%,  96.89%),  hitEff: ( 77.72%,  72.79%)\n",
    " long_strange>5GeV :       22 from       27 ( 81.5%,  82.0%)        1 clones (  4.55%), purity: ( 97.02%,  98.04%),  hitEff: ( 78.98%,  75.60%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 10th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 10th sample of minibias dataset (Samples_3412_to_6786_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 2.0, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 1875 mins 23.09 seconds.\n",
    "Average number of iterations per convergence: 78.4 iterations. \n",
    "\n",
    "ID of each event: [484, 500, 331, 523, 988, 596, 720, 517, 78, 130]\n",
    "Number of hits by event: [3532, 5482, 3783, 3865, 5879, 4014, 3927, 4267, 4377, 4168] \n",
    "Number of max_neurons by event: [7744, 19740, 11016, 13870, 17157, 17554, 12317, 11155, 16899, 9310] \n",
    "Hopfield networks runtime by event: ['02:02:11', '06:20:33', '02:00:11', '02:15:49', '04:42:04', '02:09:41', '02:08:36', '02:36:11', '03:06:13', '03:53:49'] \n",
    "\n",
    "4799 tracks including      661 ghosts ( 13.8%). Event average  13.6%\n",
    "              velo :     3273 from     5404 ( 60.6%,  61.0%)      478 clones ( 14.60%), purity: ( 97.94%,  98.00%),  hitEff: ( 69.34%,  68.78%)\n",
    "              long :     1164 from     1479 ( 78.7%,  79.3%)      317 clones ( 27.23%), purity: ( 97.94%,  98.00%),  hitEff: ( 61.57%,  60.44%)\n",
    "         long>5GeV :      735 from      902 ( 81.5%,  82.1%)      209 clones ( 28.44%), purity: ( 97.87%,  97.94%),  hitEff: ( 60.23%,  59.06%)\n",
    "      long_strange :       57 from       83 ( 68.7%,  70.2%)        8 clones ( 14.04%), purity: ( 97.93%,  98.02%),  hitEff: ( 72.42%,  70.29%)\n",
    " long_strange>5GeV :       30 from       44 ( 68.2%,  75.3%)        3 clones ( 10.00%), purity: ( 98.32%,  98.62%),  hitEff: ( 78.94%,  75.95%)\n",
    "        long_fromb :        3 from        4 ( 75.0%,  70.8%)        1 clones ( 33.33%), purity: (100.00%, 100.00%),  hitEff: ( 65.36%,  65.36%)\n",
    "   long_fromb>5GeV :        1 from        1 (100.0%, 100.0%)        1 clones (100.00%), purity: (100.00%, 100.00%),  hitEff: ( 45.00%,  45.00%)\n",
    "____________________\n",
    "\n",
    "\"\"\"\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create lists to store data for each metric\n",
    "reconstruction_efficiency_data = []\n",
    "percentage_clone_data = []\n",
    "purity_data = []\n",
    "hit_efficiency_data = []\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for section in sections:\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if len(data_lines) == 6:  # Ensure that there are enough lines in the section\n",
    "        reconstruction_efficiency_values = pattern.findall(data_lines[0])\n",
    "        percentage_clone_values = pattern.findall(data_lines[1])\n",
    "        purity_values = pattern.findall(data_lines[2])\n",
    "        hit_efficiency_values = pattern.findall(data_lines[3])\n",
    "        \n",
    "        if len(reconstruction_efficiency_values) == 1:\n",
    "            reconstruction_efficiency_data.append([f\"Experiment {i+1}\", *reconstruction_efficiency_values])\n",
    "        if len(percentage_clone_values) >= 4:\n",
    "            percentage_clone_data.append([f\"Experiment {i+1}\", *percentage_clone_values[:4]])  # Take the first 4 values\n",
    "        if len(purity_values) >= 4:\n",
    "            purity_data.append([f\"Experiment {i+1}\", *purity_values[:4]])  # Take the first 4 values\n",
    "        if len(hit_efficiency_values) >= 4:\n",
    "            hit_efficiency_data.append([f\"Experiment {i+1}\", *hit_efficiency_values[:4]])  # Take the first 4 values\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data, columns=['Experiment', 'Value1'])\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "purity_df = pd.DataFrame(purity_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data, columns=['Experiment', 'Value2', 'Value3', 'Value4', 'Value5'])\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "     Experiment  Value1\n",
      "0  Experiment 1     8.9\n",
      "1  Experiment 2     9.4\n",
      "2  Experiment 4    10.5\n",
      "\n",
      "Percentage Clone DataFrame:\n",
      "     Experiment  Value2  Value3  Value4  Value5\n",
      "0  Experiment 1    66.6   10.78   98.18   77.24\n",
      "1  Experiment 2    67.8   13.07   98.24   76.01\n",
      "2  Experiment 4    67.1   12.21   98.21   74.94\n",
      "\n",
      "Purity DataFrame:\n",
      "     Experiment  Value2  Value3  Value4  Value5\n",
      "0  Experiment 1    86.3   18.63   98.51   72.48\n",
      "1  Experiment 2    92.0   21.77   98.45   70.44\n",
      "2  Experiment 4    86.9   21.20   98.25   68.80\n",
      "\n",
      "Hit Efficiency DataFrame:\n",
      "     Experiment  Value2  Value3  Value4  Value5\n",
      "0  Experiment 1    88.7   20.69   98.28   71.20\n",
      "1  Experiment 2    91.5   24.22   98.50   69.15\n",
      "2  Experiment 4    89.5   23.73   98.15   67.32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 6th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 6th sample of minibias dataset (Samples_1819_to_2119_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1.24, 'T_decay': <function <lambda> at 0x7fbcf1eb55e0>, 'B_decay': <function <lambda> at 0x7fbcf1eb5670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 157 mins 25.64 seconds.\n",
    "Average number of iterations per convergence: 46.65 iterations. \n",
    "\n",
    "ID of each event: [395, 971, 786, 479, 10, 156, 353, 216, 498, 711]\n",
    "Number of hits by event: [1898, 1824, 2089, 1989, 1874, 1907, 2056, 1915, 1988, 2080] \n",
    "Number of max_neurons by event: [2700, 2254, 6000, 2860, 3480, 3968, 2805, 2856, 3190, 4032] \n",
    "Hopfield networks runtime by event: ['00:12:13', '00:11:45', '00:18:36', '00:14:20', '00:12:07', '00:21:40', '00:16:21', '00:14:42', '00:17:35', '00:18:00'] \n",
    "\n",
    "2158 tracks including      191 ghosts (  8.9%). Event average   8.8%\n",
    "              velo :     1586 from     2340 ( 67.8%,  66.6%)      171 clones ( 10.78%), purity: ( 98.19%,  98.18%),  hitEff: ( 77.24%,  77.24%)\n",
    "              long :      585 from      664 ( 88.1%,  86.3%)      109 clones ( 18.63%), purity: ( 98.43%,  98.51%),  hitEff: ( 72.67%,  72.48%)\n",
    "         long>5GeV :      377 from      421 ( 89.5%,  88.7%)       78 clones ( 20.69%), purity: ( 98.16%,  98.28%),  hitEff: ( 71.58%,  71.20%)\n",
    "      long_strange :       31 from       39 ( 79.5%,  76.0%)        5 clones ( 16.13%), purity: ( 97.22%,  98.07%),  hitEff: ( 71.07%,  71.67%)\n",
    " long_strange>5GeV :       18 from       23 ( 78.3%,  91.1%)        2 clones ( 11.11%), purity: ( 96.00%,  96.72%),  hitEff: ( 75.95%,  75.93%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 7th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 7th sample of minibias dataset (Samples_2121_to_2464_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7fbf79017550>, 'B_decay': <function <lambda> at 0x7fbf790175e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 234 mins 43.82 seconds.\n",
    "Average number of iterations per convergence: 50.9 iterations. \n",
    "\n",
    "ID of each event: [383, 95, 547, 217, 299, 721, 430, 105, 496, 1]\n",
    "Number of hits by event: [2366, 2319, 2399, 2418, 2250, 2157, 2123, 2180, 2372, 2422] \n",
    "Number of max_neurons by event: [2970, 3024, 3348, 4032, 3906, 2940, 3480, 3024, 3420, 3306] \n",
    "Hopfield networks runtime by event: ['00:24:05', '00:21:56', '00:27:21', '00:31:19', '00:23:19', '00:18:06', '00:17:00', '00:20:40', '00:23:50', '00:27:02'] \n",
    "\n",
    "2558 tracks including      240 ghosts (  9.4%). Event average   9.4%\n",
    "              velo :     1829 from     2714 ( 67.4%,  67.8%)      239 clones ( 13.07%), purity: ( 98.25%,  98.24%),  hitEff: ( 75.98%,  76.01%)\n",
    "              long :      657 from      728 ( 90.2%,  92.0%)      143 clones ( 21.77%), purity: ( 98.49%,  98.45%),  hitEff: ( 70.41%,  70.44%)\n",
    "         long>5GeV :      417 from      460 ( 90.7%,  91.5%)      101 clones ( 24.22%), purity: ( 98.53%,  98.50%),  hitEff: ( 69.31%,  69.15%)\n",
    "      long_strange :       28 from       34 ( 82.4%,  89.4%)        6 clones ( 21.43%), purity: ( 99.58%,  99.52%),  hitEff: ( 68.61%,  70.57%)\n",
    " long_strange>5GeV :       14 from       17 ( 82.4%,  91.4%)        5 clones ( 35.71%), purity: ( 99.25%,  98.21%),  hitEff: ( 70.74%,  63.83%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 8th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 8th sample of minibias dataset (Samples_2468_to_2853_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7ff8ab0c1550>, 'B_decay': <function <lambda> at 0x7ff8ab0c15e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 420 mins 12.23 seconds.\n",
    "Average number of iterations per convergence: 51.900000000000006 iterations. \n",
    "\n",
    "ID of each event: [387, 357, 642, 694, 413, 186, 277, 723, 388, 438]\n",
    "Number of hits by event: [2621, 2769, 2823, 2478, 2643, 2728, 2623, 2702, 2547, 2523] \n",
    "Number of max_neurons by event: [6474, 5328, 5037, 3660, 7387, 5183, 3780, 5550, 6474, 4356] \n",
    "Hopfield networks runtime by event: ['00:46:43', '00:42:54', '00:46:03', '00:27:54', '01:03:20', '00:46:26', '00:35:52', '00:41:26', '00:38:18', '00:31:12'] \n",
    "\n",
    "2862 tracks including      264 ghosts (  9.2%). Event average   9.2%\n",
    "              velo :     2092 from     3231 ( 64.7%,  64.8%)      279 clones ( 13.34%), purity: ( 98.26%,  98.27%),  hitEff: ( 75.25%,  75.17%)\n",
    "              long :      750 from      879 ( 85.3%,  85.3%)      181 clones ( 24.13%), purity: ( 98.49%,  98.50%),  hitEff: ( 69.07%,  68.54%)\n",
    "         long>5GeV :      528 from      584 ( 90.4%,  89.8%)      147 clones ( 27.84%), purity: ( 98.46%,  98.46%),  hitEff: ( 66.56%,  65.92%)\n",
    "      long_strange :       47 from       57 ( 82.5%,  81.0%)        4 clones (  8.51%), purity: ( 99.65%,  99.56%),  hitEff: ( 83.51%,  80.23%)\n",
    " long_strange>5GeV :       22 from       25 ( 88.0%,  82.4%)        4 clones ( 18.18%), purity: (100.00%, 100.00%),  hitEff: ( 78.81%,  74.02%)\n",
    "        long_fromb :        0 from        1 (  0.0%,   0.0%)        0 clones (  0.00%), purity: (  0.00%,   0.00%),  hitEff: (  0.00%,   0.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 9th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 9th sample of minibias dataset (Samples_2854_to_3405_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 723 mins 9.39 seconds.\n",
    "Average number of iterations per convergence: 56.8 iterations. \n",
    "\n",
    "ID of each event: [504, 869, 326, 797, 836, 727, 594, 753, 280, 607]\n",
    "Number of hits by event: [3304, 3008, 3350, 2957, 2940, 3375, 2879, 2992, 3157, 2951] \n",
    "Number of max_neurons by event: [7656, 5402, 7917, 6806, 5100, 8366, 6776, 5293, 7138, 6560] \n",
    "Hopfield networks runtime by event: ['01:36:38', '01:06:27', '01:26:27', '01:00:10', '01:00:00', '01:36:06', '00:54:56', '00:53:49', '01:24:13', '01:04:19'] \n",
    "\n",
    "3394 tracks including      357 ghosts ( 10.5%). Event average  10.5%\n",
    "              velo :     2466 from     3791 ( 65.0%,  67.1%)      301 clones ( 12.21%), purity: ( 98.21%,  98.21%),  hitEff: ( 74.98%,  74.94%)\n",
    "              long :      915 from     1070 ( 85.5%,  86.9%)      194 clones ( 21.20%), purity: ( 98.24%,  98.25%),  hitEff: ( 69.00%,  68.80%)\n",
    "         long>5GeV :      611 from      685 ( 89.2%,  89.5%)      145 clones ( 23.73%), purity: ( 98.14%,  98.15%),  hitEff: ( 67.43%,  67.32%)\n",
    "      long_strange :       39 from       51 ( 76.5%,  77.8%)        4 clones ( 10.26%), purity: ( 96.86%,  96.89%),  hitEff: ( 77.72%,  72.79%)\n",
    " long_strange>5GeV :       22 from       27 ( 81.5%,  82.0%)        1 clones (  4.55%), purity: ( 97.02%,  98.04%),  hitEff: ( 78.98%,  75.60%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 10th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 10th sample of minibias dataset (Samples_3412_to_6786_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 2.0, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 1875 mins 23.09 seconds.\n",
    "Average number of iterations per convergence: 78.4 iterations. \n",
    "\n",
    "ID of each event: [484, 500, 331, 523, 988, 596, 720, 517, 78, 130]\n",
    "Number of hits by event: [3532, 5482, 3783, 3865, 5879, 4014, 3927, 4267, 4377, 4168] \n",
    "Number of max_neurons by event: [7744, 19740, 11016, 13870, 17157, 17554, 12317, 11155, 16899, 9310] \n",
    "Hopfield networks runtime by event: ['02:02:11', '06:20:33', '02:00:11', '02:15:49', '04:42:04', '02:09:41', '02:08:36', '02:36:11', '03:06:13', '03:53:49'] \n",
    "\n",
    "4799 tracks including      661 ghosts ( 13.8%). Event average  13.6%\n",
    "              velo :     3273 from     5404 ( 60.6%,  61.0%)      478 clones ( 14.60%), purity: ( 97.94%,  98.00%),  hitEff: ( 69.34%,  68.78%)\n",
    "              long :     1164 from     1479 ( 78.7%,  79.3%)      317 clones ( 27.23%), purity: ( 97.94%,  98.00%),  hitEff: ( 61.57%,  60.44%)\n",
    "         long>5GeV :      735 from      902 ( 81.5%,  82.1%)      209 clones ( 28.44%), purity: ( 97.87%,  97.94%),  hitEff: ( 60.23%,  59.06%)\n",
    "      long_strange :       57 from       83 ( 68.7%,  70.2%)        8 clones ( 14.04%), purity: ( 97.93%,  98.02%),  hitEff: ( 72.42%,  70.29%)\n",
    " long_strange>5GeV :       30 from       44 ( 68.2%,  75.3%)        3 clones ( 10.00%), purity: ( 98.32%,  98.62%),  hitEff: ( 78.94%,  75.95%)\n",
    "        long_fromb :        3 from        4 ( 75.0%,  70.8%)        1 clones ( 33.33%), purity: (100.00%, 100.00%),  hitEff: ( 65.36%,  65.36%)\n",
    "   long_fromb>5GeV :        1 from        1 (100.0%, 100.0%)        1 clones (100.00%), purity: (100.00%, 100.00%),  hitEff: ( 45.00%,  45.00%)\n",
    "____________________\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create dictionaries to store data for each metric\n",
    "reconstruction_efficiency_data = {}\n",
    "percentage_clone_data = {}\n",
    "purity_data = {}\n",
    "hit_efficiency_data = {}\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for i, section in enumerate(sections):\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if len(data_lines) == 6:  # Ensure that there are enough lines in the section\n",
    "        reconstruction_efficiency_values = pattern.findall(data_lines[0])\n",
    "        percentage_clone_values = pattern.findall(data_lines[1])\n",
    "        purity_values = pattern.findall(data_lines[2])\n",
    "        hit_efficiency_values = pattern.findall(data_lines[3])\n",
    "        \n",
    "        if len(reconstruction_efficiency_values) == 1:\n",
    "            reconstruction_efficiency_data.setdefault('Experiment', []).append(f\"Experiment {i+1}\")\n",
    "            reconstruction_efficiency_data.setdefault('Value1', []).append(float(reconstruction_efficiency_values[0]))\n",
    "        if len(percentage_clone_values) >= 4:\n",
    "            percentage_clone_data.setdefault('Experiment', []).append(f\"Experiment {i+1}\")\n",
    "            percentage_clone_data.setdefault('Value2', []).append(float(percentage_clone_values[0]))\n",
    "            percentage_clone_data.setdefault('Value3', []).append(float(percentage_clone_values[1]))\n",
    "            percentage_clone_data.setdefault('Value4', []).append(float(percentage_clone_values[2]))\n",
    "            percentage_clone_data.setdefault('Value5', []).append(float(percentage_clone_values[3]))\n",
    "        if len(purity_values) >= 4:\n",
    "            purity_data.setdefault('Experiment', []).append(f\"Experiment {i+1}\")\n",
    "            purity_data.setdefault('Value2', []).append(float(purity_values[0]))\n",
    "            purity_data.setdefault('Value3', []).append(float(purity_values[1]))\n",
    "            purity_data.setdefault('Value4', []).append(float(purity_values[2]))\n",
    "            purity_data.setdefault('Value5', []).append(float(purity_values[3]))\n",
    "        if len(hit_efficiency_values) >= 4:\n",
    "            hit_efficiency_data.setdefault('Experiment', []).append(f\"Experiment {i+1}\")\n",
    "            hit_efficiency_data.setdefault('Value2', []).append(float(hit_efficiency_values[0]))\n",
    "            hit_efficiency_data.setdefault('Value3', []).append(float(hit_efficiency_values[1]))\n",
    "            hit_efficiency_data.setdefault('Value4', []).append(float(hit_efficiency_values[2]))\n",
    "            hit_efficiency_data.setdefault('Value5', []).append(float(hit_efficiency_values[3]))\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data)\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data)\n",
    "purity_df = pd.DataFrame(purity_data)\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data)\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction Efficiency DataFrame:\n",
      "     Experiment  Value1\n",
      "0  Experiment 1     8.9\n",
      "1  Experiment 2     9.4\n",
      "2  Experiment 3     9.2\n",
      "3  Experiment 4    10.5\n",
      "4  Experiment 5    13.8\n",
      "\n",
      "Percentage Clone DataFrame:\n",
      "     Experiment  Value2  Value3  Value4  Value5\n",
      "0  Experiment 1    66.6   10.78   98.18   77.24\n",
      "1  Experiment 2    67.8   13.07   98.24   76.01\n",
      "2  Experiment 3    64.8   13.34   98.27   75.17\n",
      "3  Experiment 4    67.1   12.21   98.21   74.94\n",
      "4  Experiment 5    61.0   14.60   98.00   68.78\n",
      "\n",
      "Purity DataFrame:\n",
      "     Experiment  Value2  Value3  Value4  Value5\n",
      "0  Experiment 1    86.3   18.63   98.51   72.48\n",
      "1  Experiment 2    92.0   21.77   98.45   70.44\n",
      "2  Experiment 3    85.3   24.13   98.50   68.54\n",
      "3  Experiment 4    86.9   21.20   98.25   68.80\n",
      "4  Experiment 5    79.3   27.23   98.00   60.44\n",
      "\n",
      "Hit Efficiency DataFrame:\n",
      "     Experiment  Value2  Value3  Value4  Value5\n",
      "0  Experiment 1    88.7   20.69   98.28   71.20\n",
      "1  Experiment 2    91.5   24.22   98.50   69.15\n",
      "2  Experiment 3    89.8   27.84   98.46   65.92\n",
      "3  Experiment 4    89.5   23.73   98.15   67.32\n",
      "4  Experiment 5    82.1   28.44   97.94   59.06\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Multiline string containing the text with multiple data sections\n",
    "data = \"\"\"\n",
    "\n",
    " Experiment Test of the Hopfield network on the 6th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 6th sample of minibias dataset (Samples_1819_to_2119_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1.24, 'T_decay': <function <lambda> at 0x7fbcf1eb55e0>, 'B_decay': <function <lambda> at 0x7fbcf1eb5670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 157 mins 25.64 seconds.\n",
    "Average number of iterations per convergence: 46.65 iterations. \n",
    "\n",
    "ID of each event: [395, 971, 786, 479, 10, 156, 353, 216, 498, 711]\n",
    "Number of hits by event: [1898, 1824, 2089, 1989, 1874, 1907, 2056, 1915, 1988, 2080] \n",
    "Number of max_neurons by event: [2700, 2254, 6000, 2860, 3480, 3968, 2805, 2856, 3190, 4032] \n",
    "Hopfield networks runtime by event: ['00:12:13', '00:11:45', '00:18:36', '00:14:20', '00:12:07', '00:21:40', '00:16:21', '00:14:42', '00:17:35', '00:18:00'] \n",
    "\n",
    "2158 tracks including      191 ghosts (  8.9%). Event average   8.8%\n",
    "              velo :     1586 from     2340 ( 67.8%,  66.6%)      171 clones ( 10.78%), purity: ( 98.19%,  98.18%),  hitEff: ( 77.24%,  77.24%)\n",
    "              long :      585 from      664 ( 88.1%,  86.3%)      109 clones ( 18.63%), purity: ( 98.43%,  98.51%),  hitEff: ( 72.67%,  72.48%)\n",
    "         long>5GeV :      377 from      421 ( 89.5%,  88.7%)       78 clones ( 20.69%), purity: ( 98.16%,  98.28%),  hitEff: ( 71.58%,  71.20%)\n",
    "      long_strange :       31 from       39 ( 79.5%,  76.0%)        5 clones ( 16.13%), purity: ( 97.22%,  98.07%),  hitEff: ( 71.07%,  71.67%)\n",
    " long_strange>5GeV :       18 from       23 ( 78.3%,  91.1%)        2 clones ( 11.11%), purity: ( 96.00%,  96.72%),  hitEff: ( 75.95%,  75.93%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 7th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 7th sample of minibias dataset (Samples_2121_to_2464_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7fbf79017550>, 'B_decay': <function <lambda> at 0x7fbf790175e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 234 mins 43.82 seconds.\n",
    "Average number of iterations per convergence: 50.9 iterations. \n",
    "\n",
    "ID of each event: [383, 95, 547, 217, 299, 721, 430, 105, 496, 1]\n",
    "Number of hits by event: [2366, 2319, 2399, 2418, 2250, 2157, 2123, 2180, 2372, 2422] \n",
    "Number of max_neurons by event: [2970, 3024, 3348, 4032, 3906, 2940, 3480, 3024, 3420, 3306] \n",
    "Hopfield networks runtime by event: ['00:24:05', '00:21:56', '00:27:21', '00:31:19', '00:23:19', '00:18:06', '00:17:00', '00:20:40', '00:23:50', '00:27:02'] \n",
    "\n",
    "2558 tracks including      240 ghosts (  9.4%). Event average   9.4%\n",
    "              velo :     1829 from     2714 ( 67.4%,  67.8%)      239 clones ( 13.07%), purity: ( 98.25%,  98.24%),  hitEff: ( 75.98%,  76.01%)\n",
    "              long :      657 from      728 ( 90.2%,  92.0%)      143 clones ( 21.77%), purity: ( 98.49%,  98.45%),  hitEff: ( 70.41%,  70.44%)\n",
    "         long>5GeV :      417 from      460 ( 90.7%,  91.5%)      101 clones ( 24.22%), purity: ( 98.53%,  98.50%),  hitEff: ( 69.31%,  69.15%)\n",
    "      long_strange :       28 from       34 ( 82.4%,  89.4%)        6 clones ( 21.43%), purity: ( 99.58%,  99.52%),  hitEff: ( 68.61%,  70.57%)\n",
    " long_strange>5GeV :       14 from       17 ( 82.4%,  91.4%)        5 clones ( 35.71%), purity: ( 99.25%,  98.21%),  hitEff: ( 70.74%,  63.83%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 8th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 8th sample of minibias dataset (Samples_2468_to_2853_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7ff8ab0c1550>, 'B_decay': <function <lambda> at 0x7ff8ab0c15e0>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 420 mins 12.23 seconds.\n",
    "Average number of iterations per convergence: 51.900000000000006 iterations. \n",
    "\n",
    "ID of each event: [387, 357, 642, 694, 413, 186, 277, 723, 388, 438]\n",
    "Number of hits by event: [2621, 2769, 2823, 2478, 2643, 2728, 2623, 2702, 2547, 2523] \n",
    "Number of max_neurons by event: [6474, 5328, 5037, 3660, 7387, 5183, 3780, 5550, 6474, 4356] \n",
    "Hopfield networks runtime by event: ['00:46:43', '00:42:54', '00:46:03', '00:27:54', '01:03:20', '00:46:26', '00:35:52', '00:41:26', '00:38:18', '00:31:12'] \n",
    "\n",
    "2862 tracks including      264 ghosts (  9.2%). Event average   9.2%\n",
    "              velo :     2092 from     3231 ( 64.7%,  64.8%)      279 clones ( 13.34%), purity: ( 98.26%,  98.27%),  hitEff: ( 75.25%,  75.17%)\n",
    "              long :      750 from      879 ( 85.3%,  85.3%)      181 clones ( 24.13%), purity: ( 98.49%,  98.50%),  hitEff: ( 69.07%,  68.54%)\n",
    "         long>5GeV :      528 from      584 ( 90.4%,  89.8%)      147 clones ( 27.84%), purity: ( 98.46%,  98.46%),  hitEff: ( 66.56%,  65.92%)\n",
    "      long_strange :       47 from       57 ( 82.5%,  81.0%)        4 clones (  8.51%), purity: ( 99.65%,  99.56%),  hitEff: ( 83.51%,  80.23%)\n",
    " long_strange>5GeV :       22 from       25 ( 88.0%,  82.4%)        4 clones ( 18.18%), purity: (100.00%, 100.00%),  hitEff: ( 78.81%,  74.02%)\n",
    "        long_fromb :        0 from        1 (  0.0%,   0.0%)        0 clones (  0.00%), purity: (  0.00%,   0.00%),  hitEff: (  0.00%,   0.00%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 9th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 9th sample of minibias dataset (Samples_2854_to_3405_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 1e-06, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 723 mins 9.39 seconds.\n",
    "Average number of iterations per convergence: 56.8 iterations. \n",
    "\n",
    "ID of each event: [504, 869, 326, 797, 836, 727, 594, 753, 280, 607]\n",
    "Number of hits by event: [3304, 3008, 3350, 2957, 2940, 3375, 2879, 2992, 3157, 2951] \n",
    "Number of max_neurons by event: [7656, 5402, 7917, 6806, 5100, 8366, 6776, 5293, 7138, 6560] \n",
    "Hopfield networks runtime by event: ['01:36:38', '01:06:27', '01:26:27', '01:00:10', '01:00:00', '01:36:06', '00:54:56', '00:53:49', '01:24:13', '01:04:19'] \n",
    "\n",
    "3394 tracks including      357 ghosts ( 10.5%). Event average  10.5%\n",
    "              velo :     2466 from     3791 ( 65.0%,  67.1%)      301 clones ( 12.21%), purity: ( 98.21%,  98.21%),  hitEff: ( 74.98%,  74.94%)\n",
    "              long :      915 from     1070 ( 85.5%,  86.9%)      194 clones ( 21.20%), purity: ( 98.24%,  98.25%),  hitEff: ( 69.00%,  68.80%)\n",
    "         long>5GeV :      611 from      685 ( 89.2%,  89.5%)      145 clones ( 23.73%), purity: ( 98.14%,  98.15%),  hitEff: ( 67.43%,  67.32%)\n",
    "      long_strange :       39 from       51 ( 76.5%,  77.8%)        4 clones ( 10.26%), purity: ( 96.86%,  96.89%),  hitEff: ( 77.72%,  72.79%)\n",
    " long_strange>5GeV :       22 from       27 ( 81.5%,  82.0%)        1 clones (  4.55%), purity: ( 97.02%,  98.04%),  hitEff: ( 78.98%,  75.60%)\n",
    "____________________\n",
    "\n",
    " Experiment Test of the Hopfield network on the 10th sample minibias dataset\n",
    "\n",
    "Upgraded network - Best Configuration test on 10 events from the 10th sample of minibias dataset (Samples_3412_to_6786_hits)\n",
    "Number of events: 10\n",
    "Parameters: {'random_neuron_init': True, 'binary_states': False, 'ALPHA': 1, 'BETA': 10, 'GAMMA': 10, 'narrowness': 200, 'constant_factor': 0.9, 'monotone_constant_factor': 0.9, 'T': 1e-08, 'B': 2.0, 'T_decay': <function <lambda> at 0x7f2965f645e0>, 'B_decay': <function <lambda> at 0x7f2965f64670>, 'decay_off': False, 'randomized_updates': True, 'fully_randomized_updates': False, 'maxActivation': True, 'THRESHOLD': 0.2, 'convergence_threshold': 5e-08, 'bootstrap_iters': 10, 'bootstrap_method': 'below_mean', 'smart': True, 'only_weight': False, 'max_activation': False, 'pruning_tr': 0.05}\n",
    "Total time to run all the Hopfield Networks: 1875 mins 23.09 seconds.\n",
    "Average number of iterations per convergence: 78.4 iterations. \n",
    "\n",
    "ID of each event: [484, 500, 331, 523, 988, 596, 720, 517, 78, 130]\n",
    "Number of hits by event: [3532, 5482, 3783, 3865, 5879, 4014, 3927, 4267, 4377, 4168] \n",
    "Number of max_neurons by event: [7744, 19740, 11016, 13870, 17157, 17554, 12317, 11155, 16899, 9310] \n",
    "Hopfield networks runtime by event: ['02:02:11', '06:20:33', '02:00:11', '02:15:49', '04:42:04', '02:09:41', '02:08:36', '02:36:11', '03:06:13', '03:53:49'] \n",
    "\n",
    "4799 tracks including      661 ghosts ( 13.8%). Event average  13.6%\n",
    "              velo :     3273 from     5404 ( 60.6%,  61.0%)      478 clones ( 14.60%), purity: ( 97.94%,  98.00%),  hitEff: ( 69.34%,  68.78%)\n",
    "              long :     1164 from     1479 ( 78.7%,  79.3%)      317 clones ( 27.23%), purity: ( 97.94%,  98.00%),  hitEff: ( 61.57%,  60.44%)\n",
    "         long>5GeV :      735 from      902 ( 81.5%,  82.1%)      209 clones ( 28.44%), purity: ( 97.87%,  97.94%),  hitEff: ( 60.23%,  59.06%)\n",
    "      long_strange :       57 from       83 ( 68.7%,  70.2%)        8 clones ( 14.04%), purity: ( 97.93%,  98.02%),  hitEff: ( 72.42%,  70.29%)\n",
    " long_strange>5GeV :       30 from       44 ( 68.2%,  75.3%)        3 clones ( 10.00%), purity: ( 98.32%,  98.62%),  hitEff: ( 78.94%,  75.95%)\n",
    "        long_fromb :        3 from        4 ( 75.0%,  70.8%)        1 clones ( 33.33%), purity: (100.00%, 100.00%),  hitEff: ( 65.36%,  65.36%)\n",
    "   long_fromb>5GeV :        1 from        1 (100.0%, 100.0%)        1 clones (100.00%), purity: (100.00%, 100.00%),  hitEff: ( 45.00%,  45.00%)\n",
    "____________________\n",
    "\n",
    "\"\"\"\n",
    "# Split the data into sections based on the separator\n",
    "sections = data.split(\"____________________\")\n",
    "\n",
    "# Define regular expression to match lines containing percentages\n",
    "pattern = re.compile(r\"(\\d+\\.\\d+)%\\)\")\n",
    "\n",
    "# Create dictionaries to store data for each metric\n",
    "reconstruction_efficiency_data = {'Experiment': [], 'Value1': []}\n",
    "percentage_clone_data = {'Experiment': [], 'Value2': [], 'Value3': [], 'Value4': [], 'Value5': []}\n",
    "purity_data = {'Experiment': [], 'Value2': [], 'Value3': [], 'Value4': [], 'Value5': []}\n",
    "hit_efficiency_data = {'Experiment': [], 'Value2': [], 'Value3': [], 'Value4': [], 'Value5': []}\n",
    "\n",
    "# Loop through each section, extract and store the desired values\n",
    "for i, section in enumerate(sections):\n",
    "    # Extract data lines containing percentages\n",
    "    lines = section.split(\"\\n\")\n",
    "    data_lines = [line.strip() for line in lines if \"%\" in line]\n",
    "\n",
    "    # Extract and store the desired values for each section\n",
    "    if len(data_lines) >= 6:  # Ensure that there are enough lines in the section\n",
    "        experiment_label = f\"Experiment {i+1}\"\n",
    "        \n",
    "        # Extract values using the regular expression pattern\n",
    "        reconstruction_efficiency_values = pattern.findall(data_lines[0])\n",
    "        percentage_clone_values = pattern.findall(data_lines[1])\n",
    "        purity_values = pattern.findall(data_lines[2])\n",
    "        hit_efficiency_values = pattern.findall(data_lines[3])\n",
    "        \n",
    "        if len(reconstruction_efficiency_values) == 1:\n",
    "            reconstruction_efficiency_data['Experiment'].append(experiment_label)\n",
    "            reconstruction_efficiency_data['Value1'].append(float(reconstruction_efficiency_values[0]))\n",
    "            \n",
    "        if len(percentage_clone_values) >= 4:\n",
    "            percentage_clone_data['Experiment'].append(experiment_label)\n",
    "            percentage_clone_data['Value2'].append(float(percentage_clone_values[0]))\n",
    "            percentage_clone_data['Value3'].append(float(percentage_clone_values[1]))\n",
    "            percentage_clone_data['Value4'].append(float(percentage_clone_values[2]))\n",
    "            percentage_clone_data['Value5'].append(float(percentage_clone_values[3]))\n",
    "            \n",
    "        if len(purity_values) >= 4:\n",
    "            purity_data['Experiment'].append(experiment_label)\n",
    "            purity_data['Value2'].append(float(purity_values[0]))\n",
    "            purity_data['Value3'].append(float(purity_values[1]))\n",
    "            purity_data['Value4'].append(float(purity_values[2]))\n",
    "            purity_data['Value5'].append(float(purity_values[3]))\n",
    "            \n",
    "        if len(hit_efficiency_values) >= 4:\n",
    "            hit_efficiency_data['Experiment'].append(experiment_label)\n",
    "            hit_efficiency_data['Value2'].append(float(hit_efficiency_values[0]))\n",
    "            hit_efficiency_data['Value3'].append(float(hit_efficiency_values[1]))\n",
    "            hit_efficiency_data['Value4'].append(float(hit_efficiency_values[2]))\n",
    "            hit_efficiency_data['Value5'].append(float(hit_efficiency_values[3]))\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "reconstruction_efficiency_df = pd.DataFrame(reconstruction_efficiency_data)\n",
    "percentage_clone_df = pd.DataFrame(percentage_clone_data)\n",
    "purity_df = pd.DataFrame(purity_data)\n",
    "hit_efficiency_df = pd.DataFrame(hit_efficiency_data)\n",
    "\n",
    "# Print DataFrames before saving to Excel\n",
    "print(\"Reconstruction Efficiency DataFrame:\")\n",
    "print(reconstruction_efficiency_df)\n",
    "\n",
    "print(\"\\nPercentage Clone DataFrame:\")\n",
    "print(percentage_clone_df)\n",
    "\n",
    "print(\"\\nPurity DataFrame:\")\n",
    "print(purity_df)\n",
    "\n",
    "print(\"\\nHit Efficiency DataFrame:\")\n",
    "print(hit_efficiency_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
