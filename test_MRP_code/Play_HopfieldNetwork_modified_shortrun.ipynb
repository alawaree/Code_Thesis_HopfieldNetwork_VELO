{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRP: Hopfield_v2.py\n",
    "This is the complete HN create by the MRP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hopfield v2 -> all even modules\n",
    "############################### DEPENDENCIES ##################################\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import contextlib\n",
    "import io\n",
    "import numpy as np\n",
    "import inspect\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi, atan, sin, cos, sqrt, tanh, cosh, exp, ceil\n",
    "import seaborn as sns\n",
    "from numpy.core.fromnumeric import shape\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aurel\\Documents\\GitHub\\Code_Thesis_GitHub\\Code_Thesis_GitHub\n"
     ]
    }
   ],
   "source": [
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "print(module_path)\n",
    "\n",
    "project_root = module_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from event_model import event_model as em\n",
    "from validator import validator_lite as vl\n",
    "import data_analysis.event_generator as eg\n",
    "from visual.color_map import Colormap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### CONTEXTS ##################################\n",
    "@contextlib.contextmanager\n",
    "def nostdout():\n",
    "    save_stdout = sys.stdout\n",
    "    sys.stdout = io.BytesIO()\n",
    "    yield\n",
    "    sys.stdout = save_stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### HELPER FUNCTIONS ##################################\n",
    "def get_polar_coordinates(x, y):\n",
    "    r = math.sqrt(x ** 2 + y ** 2)\n",
    "    phi = math.atan2(x, y)\n",
    "    if phi < 0:\n",
    "        phi = math.pi - phi\n",
    "    return r, phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### BODY ##########################################\n",
    "class Hopfield:\n",
    "    def __init__(self, modules: list, parameters: dict, tracks: list = None):\n",
    "        # set self variables, such as the maximum size\n",
    "        self.p = parameters\n",
    "        self.m = modules\n",
    "        self.start_T = self.p[\"T\"]\n",
    "        self.start_B = self.p[\"B\"]\n",
    "        self.N = None\n",
    "        self.N_info = None\n",
    "        self.modules_count = len(modules)\n",
    "        self.hit_counts = [len(module.hits()) for module in self.m]\n",
    "        self.neuron_count = [\n",
    "            self.hit_counts[i] * self.hit_counts[i + 1]\n",
    "            for i in range(self.modules_count - 1)\n",
    "        ]\n",
    "        self.flips = 0\n",
    "        self.max_neurons = max(self.neuron_count)\n",
    "        self.init_neurons(tracks=tracks)\n",
    "        self.init_weights()\n",
    "        self.extracted_hits = set()\n",
    "        self.extracted_tracks = []\n",
    "        self.extracted_track_states = []\n",
    "        self.energies = []\n",
    "\n",
    "    def init_neurons(self, tracks: list = None):\n",
    "        # consider hits in 2 modules as one neuron layer\n",
    "        # the neurons in N are ordered h(1,1)-h(2,1); h(1,1)-h(2,2); h(1,1)-h(2,3) etc\n",
    "        if self.p[\"random_neuron_init\"]:\n",
    "            self.N = np.random.uniform(size=(self.modules_count - 1, self.max_neurons))\n",
    "        else:\n",
    "            self.N = np.ones(shape=(self.modules_count - 1, self.max_neurons))\n",
    "        if tracks:\n",
    "            self.N = np.zeros(shape=(self.modules_count - 1, self.max_neurons))\n",
    "        for idx, nc in enumerate(self.neuron_count):\n",
    "            self.N[idx, nc:] = 0\n",
    "        self.N_info = np.zeros(shape=(self.modules_count - 1, self.max_neurons, 4))\n",
    "        for idx in range(self.modules_count - 1):\n",
    "            m1 = self.m[idx]\n",
    "            m2 = self.m[idx + 1]\n",
    "\n",
    "            for i, hit1 in enumerate(m1.hits()):\n",
    "                for j, hit2 in enumerate(m2.hits()):\n",
    "                    n_idx = i * self.hit_counts[idx + 1] + j\n",
    "                    if tracks:\n",
    "                        for t in tracks:\n",
    "                            if hit1 in t and hit2 in t:\n",
    "                                self.N[idx, n_idx] = 1\n",
    "                    # maybe we can check these angles again\n",
    "                    angle_xz = atan((hit2.x - hit1.x) / (hit2.z - hit1.z))\n",
    "                    angle_yz = atan((hit2.y - hit1.y) / (hit2.z - hit1.z))\n",
    "                    norm_dist = sqrt(\n",
    "                        (hit2.y - hit1.y) ** 2 + (hit2.x - hit1.x) ** 2\n",
    "                    ) / sqrt(\n",
    "                        (hit2.z - hit1.z) ** 2\n",
    "                    )  \n",
    "                \n",
    "                    _, r_hit1 = get_polar_coordinates(hit1.x, hit1.y)\n",
    "                    _, r_hit2 = get_polar_coordinates(hit2.x, hit2.y)\n",
    "                    monotone_dist = (r_hit2 - r_hit1) / (hit2.z - hit1.z)\n",
    "\n",
    "                    self.N_info[idx, n_idx, 0] = abs(angle_xz)\n",
    "                    self.N_info[idx, n_idx, 1] = abs(angle_yz)\n",
    "                    self.N_info[idx, n_idx, 2] = norm_dist  \n",
    "                    self.N_info[idx, n_idx, 3] = monotone_dist\n",
    "\n",
    "    def init_weights(self, neg_weights=False):\n",
    "        #### get params from the dict #######\n",
    "        alpha = self.p[\"ALPHA\"]\n",
    "        beta = self.p[\"BETA\"]\n",
    "        gamma = self.p[\"GAMMA\"]\n",
    "        #####################################\n",
    "        self.W = np.zeros(\n",
    "            shape=(self.modules_count - 2, self.max_neurons, self.max_neurons,)\n",
    "        )\n",
    "\n",
    "        # loops neuron_layer - neuron_layer weight matrices\n",
    "        for w_idx in range(self.modules_count - 2):\n",
    "            # loops hits of the module connecting the neuron layers\n",
    "            for con_idx in range(self.hit_counts[w_idx + 1]):  # m2\n",
    "                for i in range(self.hit_counts[w_idx]):  # m1\n",
    "                    ln_idx = i * self.hit_counts[w_idx + 1] + con_idx  # left_neuron_idx\n",
    "                    for j in range(self.hit_counts[w_idx + 2]):  # m3\n",
    "                        rn_idx = (\n",
    "                            con_idx * self.hit_counts[w_idx + 2] + j\n",
    "                        )  # right_neuron_idx\n",
    "\n",
    "                        # Constant term from the other group\n",
    "                        constant = (\n",
    "                            self.N_info[w_idx, ln_idx, 2]\n",
    "                            - self.N_info[w_idx + 1, rn_idx, 2]\n",
    "                        )\n",
    "                        constant = tanh(constant) * (\n",
    "                            self.p[\"narrowness\"] + 1\n",
    "                        )  # tanh to force between -1 and 1\n",
    "                        constant = (\n",
    "                            -2 * constant ** 2\n",
    "                        ) + 1  # this should be high if both terms are similar and low/penalizing if both are not similar\n",
    "                        constant = constant * self.p[\"constant_factor\"]\n",
    "                        constant = min(\n",
    "                            max(constant, -self.p[\"constant_factor\"]),\n",
    "                            self.p[\"constant_factor\"],\n",
    "                        )\n",
    "\n",
    "                        # monotone constant\n",
    "                        monotone_constant = (\n",
    "                            self.N_info[w_idx, ln_idx, 3]\n",
    "                            - self.N_info[w_idx + 1, rn_idx, 3]\n",
    "                        )\n",
    "                        monotone_constant = tanh(monotone_constant) * (\n",
    "                            self.p[\"narrowness\"] + 1\n",
    "                        )  # tanh to force between -1 and 1\n",
    "\n",
    "                        monotone_constant = (\n",
    "                            -2 * monotone_constant ** 2\n",
    "                        ) + 1  # this should be high if both terms are similar and low/penalizing if both are not similar\n",
    "\n",
    "                        monotone_constant = (\n",
    "                            monotone_constant * self.p[\"monotone_constant_factor\"]\n",
    "                        )\n",
    "                        monotone_constant = min(\n",
    "                            max(monotone_constant, -self.p[\"monotone_constant_factor\"]),\n",
    "                            self.p[\"monotone_constant_factor\"],\n",
    "                        )\n",
    "\n",
    "                        theta = abs(\n",
    "                            self.N_info[w_idx, ln_idx, 0]\n",
    "                            - self.N_info[w_idx + 1, rn_idx, 0]\n",
    "                        )\n",
    "                        phi = abs(\n",
    "                            self.N_info[w_idx, ln_idx, 1]\n",
    "                            - self.N_info[w_idx + 1, rn_idx, 1]\n",
    "                        )\n",
    "\n",
    "                        self.W[w_idx, ln_idx, rn_idx] = (\n",
    "                            alpha\n",
    "                            * ((1 - sin(theta)) ** beta)\n",
    "                            * ((1 - sin(phi)) ** gamma)\n",
    "                            + monotone_constant\n",
    "                            + constant\n",
    "                        )\n",
    "                        #    + constant # this does not work properly\n",
    "\n",
    "                        if not neg_weights:\n",
    "                            self.W[w_idx, ln_idx, rn_idx] = max(\n",
    "                                0, self.W[w_idx, ln_idx, rn_idx]\n",
    "                            )\n",
    "\n",
    "                        # maybe we can play a bit around with this\n",
    "\n",
    "    def update(self):\n",
    "        # I think here we need to look at the first neurons,\n",
    "        # then all the neurons in between as they are dependent on two other layers of neurons\n",
    "        # then the last layer of the neurons as they only dep on one side\n",
    "        update_list = []\n",
    "        for idx in range(self.modules_count - 1):\n",
    "            c1 = self.hit_counts[idx]\n",
    "            c2 = self.hit_counts[idx + 1]\n",
    "            for i in range(c1 * c2):\n",
    "                update_list.append((idx, i))\n",
    "\n",
    "        if self.p[\"randomized_updates\"]:\n",
    "            random.shuffle(update_list)\n",
    "\n",
    "        if self.p[\"fully_randomized_updates\"]:\n",
    "            for c in range(len(update_list)):\n",
    "                idx, i = random.sample(update_list, 1)[0]\n",
    "                self.update_neuron(idx, i)\n",
    "\n",
    "        else:\n",
    "            for idx, i in update_list:\n",
    "                self.update_neuron(idx, i)\n",
    "\n",
    "    def update_neuron(self, idx, i):\n",
    "        b = self.p[\"B\"]\n",
    "        t = self.p[\"T\"]\n",
    "        c1 = self.hit_counts[idx]\n",
    "        c2 = self.hit_counts[idx + 1]\n",
    "        update = 0\n",
    "        if idx > 0:\n",
    "            update += self.N[idx - 1, :].T @ self.W[idx - 1, :, i]\n",
    "        if idx < self.modules_count - 2:\n",
    "            update += self.W[idx, i, :] @ self.N[idx + 1, :]\n",
    "        if 0 < idx < self.modules_count - 1:\n",
    "            update /= 2\n",
    "        # left module and right module hit id -> current neuron connects hit lm_id with hit rn_id\n",
    "        lm_id = i // c2\n",
    "        rm_id = i % c2\n",
    "        # there can be a lot improved runtime wise with storing the sums and adj\n",
    "        # but too complicated for now\n",
    "        # all segments mapping to the hit in m1 -> the left module\n",
    "        m1h = np.sum(self.N[idx, lm_id * c2 : (lm_id + 1) * c2])\n",
    "        # all segments mapping to the hit in m2 - the right module\n",
    "        m2h = np.sum(\n",
    "            self.N[idx, : c1 * c2].reshape(c2, c1)[rm_id, :]\n",
    "        )  # correct as well...\n",
    "        # we need to subtract the neuron of the segment 2 times because we add it 2 times\n",
    "        pen = m1h + m2h - 2 * self.N[idx, i]\n",
    "        _update = 0.5 * (1 + tanh(update / t - b * pen / t))\n",
    "\n",
    "        if self.p[\"binary_states\"]:\n",
    "            # XXX some threshold... -> we dont use bina\n",
    "            if random.random() < _update:\n",
    "                self.N[idx, i] = 1\n",
    "            else:\n",
    "                self.N[idx, i] = 0\n",
    "        else:\n",
    "            self.N[idx, i] = _update\n",
    "\n",
    "    def energy(self):\n",
    "        b = self.p[\"B\"]\n",
    "\n",
    "        E = 0\n",
    "        bifurc_pen = 0\n",
    "        for idx in range(self.modules_count - 2):\n",
    "            c1 = self.hit_counts[idx]\n",
    "            c2 = self.hit_counts[idx + 1]\n",
    "            c3 = self.hit_counts[idx + 2]\n",
    "\n",
    "            # XXX: just realised we are counting most penalties too often!! -> fixed\n",
    "            f1 = 0.5\n",
    "            f2 = 0.5\n",
    "            if idx == 0:\n",
    "                f1 = 1\n",
    "            if idx == self.modules_count - 3:\n",
    "                f2 = 1\n",
    "            N1_pen = self.N[idx, : c1 * c2].reshape(c2, c1)\n",
    "            N2_pen = self.N[idx + 1, : c2 * c3].reshape(c2, c3)\n",
    "            bifurc_pen = (\n",
    "                np.sum(np.trace(N1_pen @ N1_pen.T)) * f1\n",
    "                + np.sum(np.trace(N2_pen @ N2_pen.T)) * f2\n",
    "                - np.sum(self.N[idx, :] * self.N[idx, :]) * f1\n",
    "                - np.sum(self.N[idx + 1, :] * self.N[idx + 1, :]) * f2\n",
    "            )\n",
    "\n",
    "            E += (\n",
    "                -0.5 * (self.N[idx, :].T @ self.W[idx, :, :] @ self.N[idx + 1, :])\n",
    "                + b * bifurc_pen\n",
    "            )\n",
    "        return E\n",
    "\n",
    "    def converge(self):\n",
    "        # Basically keep updating until the difference in Energy between timesteps is lower than 0.0005 (Based on Stimfple-Abele)\n",
    "        # Passaleva uses a different kind of convergence i think (4)\n",
    "        self.energies = [\n",
    "            self.energy()\n",
    "        ]  # store all energies (not fastest but maybe nice for visualisations)\n",
    "        t = 0  # timesteps\n",
    "        self.p[\"T\"] = self.start_T\n",
    "        # self.p[\"B\"] = self.start_B\n",
    "        # print(f\"N at iteration{t}:\", np.round(my_instance.N, 1))\n",
    "        self.update()\n",
    "        t += 1\n",
    "        self.energies.append(self.energy())\n",
    "        while (\n",
    "            abs(abs(self.energies[-2]) - abs(self.energies[-1]))\n",
    "            >= self.p[\"convergence_threshold\"]\n",
    "        ):\n",
    "            self.update()\n",
    "            self.energies.append(self.energy())\n",
    "            # print(f\"N at iteration{t}:\", np.round(my_instance.N, 1))\n",
    "            t += 1\n",
    "            if not self.p[\"decay_off\"]:\n",
    "                self.p[\"T\"] = self.p[\"T_decay\"](self.p[\"T\"])\n",
    "                self.p[\"B\"] = self.p[\"B_decay\"](t)\n",
    "            else:\n",
    "                pass  # keep T and B fixed\n",
    "\n",
    "        # print(\"Network Converged after \" + str(t) + \" steps\")\n",
    "        # print(\"Energy = \" + str(self.energies[-1]))\n",
    "        return self.N, self.energies[-1], t\n",
    "\n",
    "    def bootstrap_converge(self, bootstraps=50, method=\"mean\"):\n",
    "        start_time = time.time()\n",
    "        states_list = []\n",
    "        energy_list = []\n",
    "        iter_list = []\n",
    "        for i in range(bootstraps):\n",
    "\n",
    "            if self.p[\"random_neuron_init\"]:\n",
    "                # We only need to reinitialize if we randomly initialize\n",
    "                self.init_neurons()\n",
    "\n",
    "            states, energy, iters = self.converge()\n",
    "            print(\"energy: \" + str(energy))\n",
    "\n",
    "            states_list.append(states)\n",
    "            energy_list.append(energy)\n",
    "            iter_list.append(iters)\n",
    "            # print(f\"Finished {i+1}/{bootstraps} iterations\")\n",
    "\n",
    "        if method == \"minimum\":\n",
    "            # XXX: eventually we could take the lowest 20% or so\n",
    "            self.N = states_list[np.argmax(energy_list)]\n",
    "            energy_list = [np.amax(energy_list)]\n",
    "        elif method == \"below_median\":\n",
    "            median = np.median(energy_list)\n",
    "            _tmp_states = []\n",
    "            for states, e in zip(states_list, energy_list):\n",
    "                if e <= median:\n",
    "                    _tmp_states.append(states)\n",
    "            _tmp_states = np.stack(_tmp_states, axis=2)\n",
    "            self.N = np.mean(_tmp_states, axis=2)\n",
    "        elif method == \"below_mean\":\n",
    "            mean = np.mean(energy_list)\n",
    "            _tmp_states = []\n",
    "            for states, e in zip(states_list, energy_list):\n",
    "                if e <= mean:\n",
    "                    _tmp_states.append(states)\n",
    "            _tmp_states = np.stack(_tmp_states, axis=2)\n",
    "            self.N = np.mean(_tmp_states, axis=2)\n",
    "        else:\n",
    "            stacked_states = np.stack(states_list, axis=2)\n",
    "            self.N = np.mean(stacked_states, axis=2)\n",
    "\n",
    "        end_time = time.time() - start_time\n",
    "        print(\n",
    "            \"[HOPFIELD] converged network by %s after %i mins %.2f seconds; (energy: %.2f)\"\n",
    "            % (method, end_time // 60, end_time % 60, np.mean(energy_list))\n",
    "        )\n",
    "        return sum(iter_list) / len(iter_list)\n",
    "\n",
    "    def tracks(self):\n",
    "        # What the papers say:  The answer is given by the final set of active Neurons\n",
    "        #                       All sets of Neurons connected together are considered as track candidates\n",
    "        #\n",
    "        # IDEA: All neurons that share a hit and are both connected are track candidates\n",
    "        global_candidates = []\n",
    "        global_candidate_states = []\n",
    "\n",
    "        for idx in range(self.modules_count - 2):\n",
    "            candidates = []\n",
    "            candidate_states = []\n",
    "            l1 = self.hit_counts[idx]  # number of hits in module 1\n",
    "            l2 = self.hit_counts[idx + 1]\n",
    "            l3 = self.hit_counts[idx + 2]\n",
    "\n",
    "            if self.p[\"maxActivation\"]:\n",
    "                candidates = []\n",
    "                thresh = self.p[\"THRESHOLD\"]\n",
    "\n",
    "                n1_transform = self.N[idx, : l2 * l1].reshape(l1, l2).T.copy()\n",
    "                n2_transform = self.N[idx + 1, : l3 * l2].reshape(l2, l3).T.copy()\n",
    "\n",
    "                for con in range(l2):  # loop over the connection hits in module 2\n",
    "                    # XXX i try swapping these....\n",
    "                    h1_idx = np.argmax(n1_transform[con, :])\n",
    "                    h3_idx = np.argmax(n2_transform[:, con])\n",
    "\n",
    "                    if (\n",
    "                        n1_transform[con, h1_idx] < thresh\n",
    "                        or n2_transform[h3_idx, con] < thresh\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    hit1 = self.m[idx].hits()[h1_idx]\n",
    "                    hit2 = self.m[idx + 1].hits()[con]\n",
    "                    hit3 = self.m[idx + 2].hits()[h3_idx]\n",
    "                    candidates.append(em.track([hit1, hit2, hit3]))\n",
    "                    self.extracted_hits.add(hit1)\n",
    "                    self.extracted_hits.add(hit2)\n",
    "                    self.extracted_hits.add(hit3)\n",
    "                    # if we get the same state\n",
    "                    candidate_states.append(n1_transform[con, h1_idx])\n",
    "                    candidate_states.append(n2_transform[h3_idx, con])\n",
    "\n",
    "                    # XXX\n",
    "                    # this prevents the display of bifurcation?!\n",
    "                    # n1_transform[\n",
    "                    #    :, h1_idx\n",
    "                    # ] = 0  # set this hit to 0 so it's not chosen again\n",
    "                    # n2_transform[h3_idx, :] = 0\n",
    "\n",
    "            global_candidates += candidates\n",
    "            global_candidate_states += candidate_states\n",
    "\n",
    "        self.extracted_tracks = global_candidates\n",
    "        self.extracted_track_states = global_candidate_states\n",
    "\n",
    "        return global_candidates\n",
    "\n",
    "    def full_tracks(self):\n",
    "        # this will deal with stange angles!!!\n",
    "        # under the assumption that we removed bifuration completely\n",
    "        # init this active tracks with all active neurons in layer 1! -> key is the right hit\n",
    "        global_candidates = []\n",
    "        global_candidate_states = []\n",
    "        global_candidate_info = []\n",
    "        tracks = {}\n",
    "\n",
    "        for idx in range(self.modules_count - 1):\n",
    "            tracks_2 = {}\n",
    "            l1 = self.hit_counts[idx]  # number of hits in module 1 / L\n",
    "            l2 = self.hit_counts[idx + 1]  # number of hits in module 2 / R\n",
    "\n",
    "            tr = self.p[\"THRESHOLD\"]\n",
    "            for segment in range(l1 * l2):\n",
    "                if self.N[idx, segment] < tr:\n",
    "                    continue\n",
    "\n",
    "                r_hit = self.m[idx + 1].hits()[segment % l2]\n",
    "                l_hit = self.m[idx].hits()[segment // l2]\n",
    "\n",
    "                if l_hit in tracks.keys():\n",
    "                    (track, states, angle) = tracks[l_hit]\n",
    "\n",
    "                    track = track + [r_hit]\n",
    "                    states = states + [self.N[idx, segment]]\n",
    "                    info = angle + [self.N_info[idx, segment, :]]\n",
    "                    del tracks[l_hit]\n",
    "\n",
    "                    self.extracted_hits.add(r_hit)\n",
    "                    tracks_2[r_hit] = (track, states, info)\n",
    "\n",
    "                else:\n",
    "                    track = [l_hit, r_hit]\n",
    "                    states = [self.N[idx, segment]]\n",
    "                    info = [self.N_info[idx, segment, :]]\n",
    "\n",
    "                    tracks_2[r_hit] = (track, states, info)\n",
    "                    self.extracted_hits.add(r_hit)\n",
    "                    self.extracted_hits.add(l_hit)\n",
    "\n",
    "            for _, value in tracks.items():\n",
    "                (track, states, info) = value\n",
    "                global_candidates = global_candidates + [track]\n",
    "                global_candidate_states = global_candidate_states + [states]\n",
    "                global_candidate_info = global_candidate_info + [info]\n",
    "            tracks = tracks_2\n",
    "\n",
    "        for _, value in tracks.items():\n",
    "            (track, states, info) = value\n",
    "            global_candidates = global_candidates + [track]\n",
    "            global_candidate_states = global_candidate_states + [states]\n",
    "            global_candidate_info = global_candidate_info + [info]\n",
    "\n",
    "        # here comes the function of 'pruning...' maybe i need to store more info for doing that!!!\n",
    "        global_candidates = self.prune_tracks(global_candidates, global_candidate_info)\n",
    "\n",
    "        global_candidates = [em.track(hits) for hits in global_candidates]\n",
    "        self.extracted_tracks = global_candidates\n",
    "        self.extracted_track_states = global_candidate_states\n",
    "\n",
    "        return global_candidates\n",
    "\n",
    "    # tr 0.1 seems decent for sum of info differences...\n",
    "    # we could look more carully into a criterion for this on big instances but here is fine...\n",
    "    # XXX: need to check that method properly again\n",
    "    def prune_tracks(self, tracks, track_infos):\n",
    "        tr = self.p[\"pruning_tr\"]\n",
    "        out_tracks = []\n",
    "        for track, info in zip(tracks, track_infos):\n",
    "            num_hits = len(track)\n",
    "            if num_hits < 3:  # sorting out the tracks that are not relevant\n",
    "                continue\n",
    "            # only if len> 6 need to think about splitting!!!\n",
    "            cand = [track[0], track[1]]\n",
    "            cand_info = info[0]\n",
    "            for idx in range(1, num_hits - 1):\n",
    "                # if abs(cand_info[3] - info[idx][3]) < tr:\n",
    "                if sum(abs(cand_info - info[idx])) < tr:\n",
    "                    cand = cand + [track[idx + 1]]\n",
    "                else:\n",
    "                    if len(cand) > 2:\n",
    "                        out_tracks = out_tracks + [cand]\n",
    "                    cand = [track[idx], track[idx + 1]]\n",
    "                cand_info = info[idx]\n",
    "\n",
    "            if len(cand) > 2:\n",
    "                out_tracks = out_tracks + [cand]\n",
    "        return out_tracks\n",
    "\n",
    "    def mark_bifurcation(self):\n",
    "        zero = True\n",
    "        max_activation = self.p[\"max_activation\"]\n",
    "        smart = self.p[\"smart\"]\n",
    "        if max_activation:\n",
    "            zero = False\n",
    "        if smart:\n",
    "            zero = False\n",
    "            max_activation = False\n",
    "\n",
    "        tr = self.p[\"THRESHOLD\"]\n",
    "        self.N[self.N <= tr] = 0\n",
    "\n",
    "        # search for bifurcation neurons\n",
    "        for idx in range(self.modules_count - 1):\n",
    "            # so basically we visit all neurons in one layer and check for neurons where the activation is bigger than tr\n",
    "            # then we check all adjacent neurons for activation and check how many are higher than the treshold\n",
    "            # for each segment we look wether there is bifurcation on the left or right hit\n",
    "            c1 = self.hit_counts[idx]\n",
    "            c2 = self.hit_counts[idx + 1]\n",
    "\n",
    "            for segment in range(c1 * c2):\n",
    "                if self.N[idx, segment] < tr:\n",
    "                    continue\n",
    "                r_hit = segment % c2\n",
    "                l_hit = segment // c2\n",
    "\n",
    "                # left-right bifurction\n",
    "                activation_mask = self.N[idx, : c1 * c2].reshape(c1, c2)[:, r_hit] > tr\n",
    "                if sum(activation_mask) > 1:  # we have bifuct into the right hit\n",
    "                    affected_neurons = []\n",
    "                    for i in range(c1):  # loop over all nerons affected by bifurc\n",
    "                        if activation_mask[i]:\n",
    "                            # well here are the bifurcation things detected. here we would need to come up with a smart way to resolve it\n",
    "                            if zero:\n",
    "                                self.N[idx, (i * c2) + r_hit] = 0\n",
    "                            else:\n",
    "                                affected_neurons = affected_neurons + [(i * c2) + r_hit]\n",
    "                    if smart:\n",
    "                        # simple rule -> when bifurc is detected on right side -> we look next active neurons going out\n",
    "                        # and promote the ones where the weight is high... (angle diff is low)\n",
    "                        # if next neuron layer exist!!!\n",
    "                        if idx < self.modules_count - 2:  # can check to the right\n",
    "                            c3 = self.hit_counts[idx + 2]\n",
    "                            activation_mask_2 = (\n",
    "                                self.N[idx + 1, : c2 * c3].reshape(c2, c3)[r_hit, :]\n",
    "                                > tr\n",
    "                            )\n",
    "                            affected_neurons_2 = []\n",
    "                            for i in range(\n",
    "                                c3\n",
    "                            ):  # loop over all nerons affected by bifurc\n",
    "                                if activation_mask_2[i]:\n",
    "                                    affected_neurons_2 = affected_neurons_2 + [\n",
    "                                        c3 * r_hit + i\n",
    "                                    ]\n",
    "                            if len(affected_neurons_2) > 0:\n",
    "                                max_val = 0\n",
    "                                max_l = None\n",
    "                                max_r = None\n",
    "                                for e in affected_neurons:\n",
    "                                    for j in affected_neurons_2:\n",
    "                                        c = (\n",
    "                                            self.N[idx, e]\n",
    "                                            * self.W[idx, e, j]\n",
    "                                            * self.N[idx + 1, j]\n",
    "                                        )\n",
    "                                        if self.p[\"only_weight\"]:\n",
    "                                            c = self.W[idx, e, j]\n",
    "                                        if c > max_val:\n",
    "                                            max_l = e\n",
    "                                            max_r = j\n",
    "                                            max_val = c\n",
    "                                    self.N[idx, e] = 0\n",
    "                                for j in affected_neurons_2:\n",
    "                                    self.N[idx + 1, j] = 0\n",
    "                                if max_r is not None and max_l is not None:\n",
    "                                    self.N[idx, max_l] = 1\n",
    "                                    self.N[idx + 1, max_r] = 1\n",
    "                            else:\n",
    "                                max_activation = True\n",
    "                        else:\n",
    "                            max_activation = True\n",
    "\n",
    "                    if max_activation:\n",
    "                        max_activation = self.N[idx, affected_neurons[0]]\n",
    "                        max_id = affected_neurons[0]\n",
    "                        for e in affected_neurons:\n",
    "                            if self.N[idx, e] >= max_activation:\n",
    "                                max_id = e\n",
    "                                max_activation = self.N[idx, e]\n",
    "\n",
    "                            self.N[idx, e] = 0\n",
    "\n",
    "                        self.N[idx, max_id] = 1\n",
    "                    if smart:\n",
    "                        max_activation = False\n",
    "\n",
    "                # right-left bifurcation\n",
    "                activation_mask = self.N[idx, : c1 * c2].reshape(c1, c2)[l_hit, :] > tr\n",
    "                if sum(activation_mask) > 1:\n",
    "                    affected_neurons = []\n",
    "                    affected_neurons_2 = []\n",
    "                    for i in range(c2):\n",
    "                        if activation_mask[i]:\n",
    "                            if zero:\n",
    "                                self.N[idx, (l_hit * c2) + i] = 0\n",
    "                            else:\n",
    "                                affected_neurons = affected_neurons + [(l_hit * c2) + i]\n",
    "                    if smart:  # i know there can be only one neuron on the right.\n",
    "                        if idx > 0:\n",
    "                            # can check here only for the first active neuron, because we removed\n",
    "                            # the other bifurcation in the previous iteration\n",
    "                            c0 = self.hit_counts[idx - 1]\n",
    "                            activation_mask_2 = (\n",
    "                                self.N[idx - 1, : c0 * c1].reshape(c0, c1)[:, l_hit]\n",
    "                                > tr\n",
    "                            )\n",
    "                            if sum(activation_mask_2) > 0:\n",
    "                                for i in range(\n",
    "                                    c0  # this was c0...\n",
    "                                ):  # loop over all nerons affected by bifurc\n",
    "                                    if activation_mask_2[i]:\n",
    "                                        affected_neurons_2 = affected_neurons_2 + [\n",
    "                                            c1 * i + l_hit\n",
    "                                        ]\n",
    "\n",
    "                                if len(affected_neurons_2) > 0:\n",
    "                                    max_val = 0\n",
    "                                    max_l = None\n",
    "                                    max_r = None\n",
    "                                    for e in affected_neurons_2:\n",
    "                                        for j in affected_neurons:\n",
    "                                            c = (\n",
    "                                                self.N[idx - 1, e]\n",
    "                                                * self.W[idx - 1, e, j]\n",
    "                                                * self.N[idx, j]\n",
    "                                            )\n",
    "                                            if self.p[\"only_weight\"]:\n",
    "                                                c = self.W[idx - 1, e, j]\n",
    "                                            if c > max_val:\n",
    "                                                max_l = e\n",
    "                                                max_r = j\n",
    "                                                max_val = c\n",
    "                                        self.N[idx - 1, e] = 0\n",
    "                                for e in affected_neurons:\n",
    "                                    self.N[idx, j] = 0\n",
    "                                if max_r is not None and max_l is not None:\n",
    "                                    self.N[idx - 1, max_l] = 1\n",
    "                                    self.N[idx, max_r] = 1\n",
    "                            else:\n",
    "                                max_activation = True\n",
    "                        else:\n",
    "                            max_activation = True\n",
    "                        pass\n",
    "\n",
    "                    if max_activation:\n",
    "                        # check to the left or max score\n",
    "                        max_activation = self.N[idx, affected_neurons[0]]\n",
    "                        max_id = affected_neurons[0]\n",
    "                        for e in affected_neurons:\n",
    "                            if self.N[idx, e] >= max_activation:\n",
    "                                max_id = e\n",
    "                                max_activation = self.N[idx, e]\n",
    "                            self.N[idx, e] = 0\n",
    "                        self.N[idx, max_id] = 1\n",
    "\n",
    "        # converged, averaged neuron state\n",
    "        # what do we want to do -> search all neurons wether there is bifurcation\n",
    "        # how to search this, by the indices... and then we store it as a combination of hit id_s, and which side of this element the bifurcation occurs\n",
    "        # bifiurcation is stored as a list of hit ids, with\n",
    "\n",
    "    def show_all_tracks(self, threshold=None, show_states=False):\n",
    "        # Creates a colormap from blue to red for small to large values respectively\n",
    "        c_map = Colormap(0, 1, 2 / 3.0, 0)\n",
    "        c = []\n",
    "        tracks = []\n",
    "        for idx in range(self.modules_count - 1):\n",
    "            m1 = self.m[idx]\n",
    "            m2 = self.m[idx + 1]\n",
    "\n",
    "            for i, hit1 in enumerate(m1.hits()):\n",
    "                for j, hit2 in enumerate(m2.hits()):\n",
    "                    n_idx = i * self.hit_counts[idx + 1] + j\n",
    "                    if threshold:\n",
    "                        if self.N[idx, n_idx] >= threshold:\n",
    "                            tracks.append(em.track([hit1, hit2]))\n",
    "                            if show_states:\n",
    "                                c.append(c_map.get_color_rgb(self.N[idx, n_idx]))\n",
    "                        continue\n",
    "                    if show_states:\n",
    "                        c.append(c_map.get_color_rgb(self.N[idx, n_idx]))\n",
    "                    tracks.append(em.track([hit1, hit2]))\n",
    "        eg.plot_tracks_and_modules(tracks, self.m, colors=c)\n",
    "\n",
    "    def tracks_with_hit(self, hit):\n",
    "        return [track for track in self.extracted_tracks if hit in track.hits]\n",
    "\n",
    "    def print_neurons(self):\n",
    "        n = len(self.N)\n",
    "        for i in range(n):\n",
    "            m = int(sqrt(len(self.N[i])))\n",
    "            for j in range(m):\n",
    "                print(f\"m{i+1}h{j+1}: {self.N[i, (j*m):((j+1)*m)]}\")\n",
    "\n",
    "    def plot_network_results(self, show_states=False):\n",
    "        if show_states:\n",
    "            # Creates a colormap from blue to red for small to large values respectively\n",
    "            c_map = Colormap(0, 1, 2 / 3.0, 0)\n",
    "            colors = []\n",
    "            [colors.append(c_map.get_color_rgb(v)) for v in self.extracted_track_states]\n",
    "            eg.plot_tracks_and_modules(\n",
    "                self.extracted_tracks,\n",
    "                self.m,\n",
    "                colors=colors,\n",
    "                title=\"Hopfield Output with states\",\n",
    "            )\n",
    "        else:\n",
    "            eg.plot_tracks_and_modules(\n",
    "                self.extracted_tracks, self.m, title=\"Hopfield Output\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_instance(\n",
    "    even=True, num_modules=10, plot_events=False, num_tracks=3, save_to_file: str = None\n",
    "):\n",
    "    if num_modules > 26:\n",
    "        num_modules = 26\n",
    "    elif num_modules < 3 or type(num_modules) != int:\n",
    "        num_modules = 3\n",
    "\n",
    "    # Generating a test event to work with\n",
    "    tracks = eg.generate_test_tracks(\n",
    "        allowed_modules=[i * 2 for i in range(num_modules)],\n",
    "        num_test_events=1,\n",
    "        num_tracks=num_tracks,\n",
    "        reconstructable_tracks=True,\n",
    "    )[0]\n",
    "\n",
    "    if save_to_file:\n",
    "        eg.write_tracks(tracks, save_to_file)\n",
    "\n",
    "    modules = eg.tracks_to_modules(tracks)\n",
    "    if plot_events:\n",
    "        eg.plot_tracks_and_modules(tracks, modules, title=\"Generated Instance\")\n",
    "    return modules, tracks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_instance(file_name, plot_events=False):\n",
    "    tracks = eg.read_tracks(file_name)\n",
    "    modules = eg.tracks_to_modules(tracks)\n",
    "    if plot_events:\n",
    "        eg.plot_tracks_and_modules(tracks, modules, title=\"Generated Instance\")\n",
    "    return modules, tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_event(file_name, plot_event=False):\n",
    "    f = open(file_name)\n",
    "    json_data_event = json.loads(f.read())\n",
    "\n",
    "    ev = em.event(json_data_event, read_tracks=True)\n",
    "\n",
    "    modules = ev.modules\n",
    "    tracks = ev.real_tracks\n",
    "\n",
    "    if plot_event:\n",
    "        eg.plot_tracks_and_modules(tracks, modules, title=\"Loaded Event\")\n",
    "\n",
    "    modules_even = []\n",
    "    modules_odd = []\n",
    "\n",
    "    for i in range(len(modules)):\n",
    "        if i % 2 == 0:\n",
    "            modules_even.append(modules[i])\n",
    "        else:\n",
    "            modules_odd.append(modules[i])\n",
    "\n",
    "    return json_data_event, (modules_even, modules_odd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_events(file_name, parameters, nr_events=1, plot_event=False, output_file=None):\n",
    "\n",
    "    json_data_all_events = []\n",
    "    all_tracks = []\n",
    "    iter_even = 1\n",
    "    iter_odd = 1\n",
    "\n",
    "    all_events = [i for i in range(1000)]\n",
    "    random.seed(40)\n",
    "    random.shuffle(all_events)\n",
    "    count = 0\n",
    "    j = 0\n",
    "    \n",
    "    while count < nr_events:\n",
    "        i = all_events[j]\n",
    "        j += 1\n",
    "\n",
    "        try:\n",
    "            size = os.path.getsize(file_name + str(i) + \".json\")\n",
    "            print(\"[INFO] Evaluate Event: %s\" % file_name + str(i))\n",
    "            json_data_event, modules = load_event(\n",
    "                file_name + str(i) + \".json\", plot_event=False\n",
    "            )\n",
    "            max_neurons = 0\n",
    "            last = 0\n",
    "            for m in modules[0]:\n",
    "                n_hits = len(m.hits())\n",
    "                if last * n_hits > max_neurons:\n",
    "                    max_neurons = last * n_hits\n",
    "                last = n_hits\n",
    "            last = 0\n",
    "            for m in modules[1]:\n",
    "                n_hits = len(m.hits())\n",
    "                if last * n_hits > max_neurons:\n",
    "                    max_neurons = last * n_hits\n",
    "                last = n_hits\n",
    "            #if max_neurons > 2200:\n",
    "            #    continue\n",
    "\n",
    "            print(f\"\\nstarting instance {count+1} out of {nr_events}\\n\")\n",
    "            print(f'Number of max neurons: {max_neurons}')\n",
    "            start_time = time.time()\n",
    "            even_hopfield = Hopfield(modules=modules[0], parameters=parameters)\n",
    "            odd_hopfield = Hopfield(modules=modules[1], parameters=parameters)\n",
    "            end_time = time.time() - start_time\n",
    "            print(\n",
    "                \"[INFO] Hopfield Networks initialized in %i mins %.2f seconds\"\n",
    "                % (end_time // 60, end_time % 60)\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                iter_even = even_hopfield.bootstrap_converge(\n",
    "                    bootstraps=parameters[\"bootstrap_iters\"],\n",
    "                    method=parameters[\"bootstrap_method\"],\n",
    "                )\n",
    "                iter_odd = odd_hopfield.bootstrap_converge(\n",
    "                    bootstraps=parameters[\"bootstrap_iters\"],\n",
    "                    method=parameters[\"bootstrap_method\"],\n",
    "                )\n",
    "\n",
    "                start_time = time.time()\n",
    "                even_hopfield.mark_bifurcation()\n",
    "                odd_hopfield.mark_bifurcation()\n",
    "                even_tracks = even_hopfield.full_tracks()\n",
    "                odd_tracks = odd_hopfield.full_tracks()\n",
    "                event_tracks = even_tracks + odd_tracks\n",
    "                end_time = time.time() - start_time\n",
    "                print(\n",
    "                    \"[INFO] tracks extracted in %i mins %.2f seconds\"\n",
    "                    % (end_time // 60, end_time % 60)\n",
    "                )\n",
    "\n",
    "                json_data_all_events.append(json_data_event)\n",
    "                all_tracks.append(event_tracks)\n",
    "\n",
    "                if plot_event:\n",
    "                    even_hopfield.plot_network_results()\n",
    "                    odd_hopfield.plot_network_results()\n",
    "\n",
    "                count = count + 1\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    start_time = time.time()\n",
    "    if output_file:\n",
    "        print(output_file)\n",
    "        sys.stdout = open(output_file, \"a\")\n",
    "        print(f\"Average number of iterations per convergence: {(iter_even+iter_odd)/2}\")\n",
    "        vl.validate_print(json_data_all_events, all_tracks, return_data=True)\n",
    "        print(\"____________________\")\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = sys.__stdout__\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # we could check how many tracks acutally cross the detector sides i guess to identify where some clones come from...\n",
    "    print(\n",
    "        \"[INFO] validation excecuted in %i mins %.2f seconds\"\n",
    "        % (end_time // 60, end_time % 60)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(network, tracks):\n",
    "    true_network = Hopfield(modules=network.m, parameters=network.p, tracks=tracks)\n",
    "    return ((network.N - true_network.N) ** 2).mean(axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_experiment(exp_name, exp_num, desc, p, event_file_name, nr_events):\n",
    "    f = open(project_root + \"/algorithms/experiments/\" + exp_name + \".txt\", \"a\")\n",
    "    f.write(\n",
    "        f\"\\n Experiment {exp_num}\\n\\n{desc}\\nNumber of events: {nr_events}\\nParameters: {p}\\n\"\n",
    "    )\n",
    "    f.close()\n",
    "    \n",
    "    evaluate_events(\n",
    "        project_root + event_file_name,\n",
    "        p,\n",
    "        nr_events,\n",
    "        True,\n",
    "        project_root + \"/algorithms/experiments/\" + exp_name + \".txt\",\n",
    "    )\n",
    "\n",
    "    f = open(project_root + \"/algorithms/experiments/\" + exp_name + \".txt\", \"a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #################### PARAMETERS #######################\n",
    "    parameters = {\n",
    "        ### NEURONS ###\n",
    "        \"random_neuron_init\": True,\n",
    "        \"binary_states\": False,  # try it out once maybe but scrap it\n",
    "        ### WEIGHTS ###\n",
    "        \"ALPHA\": 1,\n",
    "        \"BETA\": 10,\n",
    "        \"GAMMA\": 10,\n",
    "        \"narrowness\": 200,\n",
    "        \"constant_factor\": 0.9,\n",
    "        \"monotone_constant_factor\": 0.9,\n",
    "        #### UPDATE ###\n",
    "        \"T\": 1e-8,  # try to experiment with these rather\n",
    "        \"B\": 1e-6,  # try to experiment with these rather\n",
    "        \"T_decay\": lambda t: max(1e-8, t * 0.01),  # try to remove these\n",
    "        \"B_decay\": lambda t: max(1e-4, t * 0.04),  # try to remove these\n",
    "        \"decay_off\": False,  # using this\n",
    "        \"randomized_updates\": True,\n",
    "        \"fully_randomized_updates\": False,\n",
    "        #### THRESHOLD ###\n",
    "        \"maxActivation\": True,\n",
    "        \"THRESHOLD\": 0.2,\n",
    "        ##### CONVERGENCE ###\n",
    "        \"convergence_threshold\": 0.00000005,\n",
    "        \"bootstrap_iters\": 10,\n",
    "        \"bootstrap_method\": \"below_mean\",\n",
    "        ###### BIFURC REMOVAL #####\n",
    "        \"smart\": True,\n",
    "        \"only_weight\": False,\n",
    "        \"max_activation\": False,\n",
    "        ###### Track prunning #######\n",
    "        # here we could set the threshold\n",
    "        \"pruning_tr\": 0.05,\n",
    "    }\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Evaluate Event: c:\\Users\\aurel\\Documents\\GitHub\\Code_Thesis_GitHub\\Code_Thesis_GitHub/datasets/samples/minibias/Samples_2784_to_4492_neurons/velo_event_95\n",
      "\n",
      "starting instance 1 out of 10\n",
      "\n",
      "Number of max neurons: 3024\n",
      "[INFO] Hopfield Networks initialized in 0 mins 27.33 seconds\n",
      "energy: -836.6775716735693\n",
      "energy: -650.8679251801153\n",
      "energy: -691.0321609981625\n",
      "energy: -568.9095695552656\n",
      "energy: -710.3701786988511\n",
      "energy: -697.3064209932975\n",
      "energy: -671.1593833910263\n",
      "energy: -688.1970066944536\n",
      "energy: -652.611295426611\n",
      "energy: -665.3605731570419\n",
      "[HOPFIELD] converged network by below_mean after 65 mins 54.35 seconds; (energy: -683.25)\n",
      "energy: -639.7025072825379\n",
      "energy: -693.4107383000969\n",
      "energy: -667.7084784556628\n",
      "energy: -699.9321733313259\n",
      "energy: -694.2150305494935\n",
      "energy: -685.2734407269695\n",
      "energy: -588.7378397623922\n",
      "energy: -741.5751075440566\n"
     ]
    }
   ],
   "source": [
    "###################################################################\n",
    "save_experiment(\n",
    "        \"aurelie_experiments_10_02\",\n",
    "        \"B equal 100*T\",\n",
    "        \"Modified network - Best Configuration 10 events from 'Samples_2784_to_4492_neurons' Test with 0.05 Pruning Threshold, no more constraint on the neurons \",\n",
    "        parameters,\n",
    "        \"/datasets/samples/minibias/Samples_2784_to_4492_neurons/velo_event_\",\n",
    "        10,\n",
    "    )\n",
    "exit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with event 504 in mininbias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "01fe50a16b4ce4d8d34632692592e1bc62ea71f5ccf21d36838697cffecce7a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
